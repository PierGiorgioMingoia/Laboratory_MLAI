{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copia di Homework2-MLDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9O3aM3Tb28q",
        "outputId": "8c5cbef3-101b-46c9-d2fd-00bc8488efb9"
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.3.1\n",
            "  Using cached https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.18.5)\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.3.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "Successfully installed torch-1.3.1\n",
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.5)\n",
            "Collecting torch==1.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.3.1\n",
            "    Uninstalling torch-1.3.1:\n",
            "      Successfully uninstalled torch-1.3.1\n",
            "Successfully installed torch-1.4.0\n",
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (7.0.0.post3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 102 # 101 + 1: There is am extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.01            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 40      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc"
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fqRlrw5rDhe"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfVq_uDHLbsR",
        "outputId": "64bfe4f3-02b8-46dc-f304-e5d9d02d306d"
      },
      "source": [
        "import random\n",
        "\n",
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Caltech101'):\n",
        "  !git clone https://github.com/PierGiorgioMingoia/Homework2-Caltech101.git\n",
        "  !mv 'Homework2-Caltech101' 'Caltech101'\n",
        "\n",
        "DATA_DIR = 'Caltech101/101_ObjectCategories'\n",
        "from Caltech101.caltech_dataset import Caltech\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = Caltech(DATA_DIR, split='train',  transform=train_transform)\n",
        "test_dataset = Caltech(DATA_DIR, split='test', transform=eval_transform)\n",
        "print(len(train_dataset))\n",
        "indexes = list(range(5784))\n",
        "indexes= random.sample(indexes, len(indexes))\n",
        "print(indexes)\n",
        "train_indexes = indexes[:int(len(indexes)/2)]# split the indices for your train split\n",
        "val_indexes = indexes[int(len(indexes)/2):]# split the indices for your val split\n",
        "\n",
        "\n",
        "\n",
        "val_dataset = Subset(train_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5784\n",
            "[2390, 1803, 2473, 1241, 3846, 2500, 5000, 1647, 97, 4881, 3470, 4451, 3445, 946, 4442, 3823, 530, 1433, 1422, 3613, 4281, 1589, 5415, 3323, 1289, 4087, 937, 2684, 518, 478, 5090, 2837, 1542, 826, 4015, 5678, 892, 2032, 5693, 1829, 3604, 4508, 5165, 1204, 3253, 3299, 2491, 1337, 5598, 2813, 4270, 2893, 4211, 4762, 4470, 3148, 3248, 4902, 5701, 5710, 1496, 16, 1392, 5222, 4064, 3554, 712, 2327, 1833, 1742, 3634, 3754, 5484, 3688, 873, 2960, 3914, 1288, 4977, 2248, 1544, 2384, 3907, 3621, 1891, 3412, 5652, 5161, 3405, 408, 788, 213, 992, 3059, 4837, 3216, 392, 3921, 4402, 5676, 5565, 2639, 4864, 5228, 4236, 5550, 702, 5392, 3905, 5018, 2273, 5523, 3643, 2641, 5047, 3140, 768, 3410, 3169, 4603, 316, 1953, 973, 431, 2616, 3245, 2607, 2649, 143, 2095, 4491, 517, 1303, 744, 4457, 2959, 2627, 4726, 3338, 3041, 3385, 2603, 698, 3490, 1716, 4833, 465, 2360, 1968, 5083, 4692, 4392, 2806, 5648, 979, 3172, 4804, 1176, 4760, 152, 3144, 2160, 652, 1147, 2290, 4117, 2510, 3576, 5140, 514, 4689, 2272, 4376, 1543, 1315, 3139, 3448, 1909, 988, 1913, 1451, 5638, 2958, 4232, 1756, 2624, 1027, 2101, 5529, 4498, 1985, 1576, 297, 1142, 5189, 5108, 858, 198, 2150, 3963, 2834, 669, 4042, 1801, 3565, 29, 1361, 1039, 1815, 5303, 3230, 1521, 3207, 3966, 705, 2657, 2232, 715, 1587, 2205, 4803, 4740, 393, 904, 1102, 5072, 5690, 2129, 1975, 2437, 5328, 4986, 4890, 3444, 5050, 4468, 2572, 2889, 5167, 5312, 3623, 1081, 3214, 564, 1132, 497, 875, 4161, 1407, 4950, 2704, 3500, 2245, 2719, 2112, 1910, 3919, 3237, 2749, 479, 4946, 4081, 1000, 3784, 5344, 3927, 403, 2574, 5639, 3644, 4277, 4797, 3787, 4887, 3816, 908, 4675, 365, 4128, 3968, 4531, 3511, 5064, 545, 2467, 4936, 2447, 4875, 3573, 1585, 2212, 5514, 1086, 2506, 3684, 2312, 960, 603, 2842, 4661, 2423, 2923, 4119, 167, 4251, 424, 5675, 4072, 4115, 1578, 2841, 5101, 2176, 4447, 2588, 2193, 1154, 570, 772, 1830, 2658, 5054, 751, 4482, 3615, 3562, 4016, 4217, 2782, 2348, 1989, 2123, 39, 981, 2180, 3421, 5460, 1333, 243, 2338, 4737, 3321, 2761, 4471, 1947, 5451, 5130, 3217, 969, 1338, 2514, 1216, 4960, 2282, 2061, 1067, 1530, 78, 4672, 4049, 5326, 2594, 5078, 5420, 645, 5431, 5114, 2710, 280, 2740, 3814, 5013, 1922, 4926, 3369, 3556, 4460, 2020, 1378, 5193, 1137, 2309, 2318, 305, 5281, 523, 5006, 5105, 2322, 3358, 5183, 140, 5221, 361, 5709, 4092, 927, 4856, 3946, 4850, 5405, 958, 3723, 1522, 86, 5255, 333, 4965, 720, 2220, 2165, 492, 67, 480, 3938, 3931, 5036, 4527, 2109, 814, 861, 4904, 4169, 1693, 4994, 1197, 1721, 1178, 2048, 885, 1284, 5380, 3859, 4472, 4592, 4561, 1438, 2287, 2830, 4452, 5584, 4086, 2703, 4829, 3360, 414, 179, 1462, 5363, 1920, 4203, 5254, 516, 1484, 2642, 4817, 5201, 4007, 3690, 2530, 1340, 1701, 4445, 320, 464, 5027, 442, 3790, 4985, 3060, 4745, 207, 2152, 5276, 4184, 33, 3616, 1614, 507, 4396, 651, 2571, 5413, 189, 2203, 5501, 3012, 1674, 1917, 325, 1682, 1861, 3185, 5597, 2047, 5641, 1356, 5582, 4133, 5283, 3066, 5151, 1112, 1329, 2178, 657, 4826, 1115, 2736, 45, 1816, 4141, 4395, 806, 4493, 2746, 3894, 900, 2697, 2865, 4204, 3272, 1492, 830, 728, 4312, 1494, 5155, 5395, 2325, 3460, 2311, 5419, 4415, 5704, 5340, 3016, 3945, 52, 5308, 1948, 3822, 4017, 5522, 2852, 5300, 1212, 4291, 808, 4567, 956, 3789, 848, 2826, 5530, 5692, 5465, 3552, 353, 1963, 688, 5251, 3974, 3452, 2241, 1321, 64, 3006, 2961, 4354, 3201, 248, 3044, 3761, 3330, 2839, 3363, 463, 3858, 577, 2494, 4485, 5321, 477, 5508, 907, 3773, 2146, 1548, 1032, 731, 4356, 4614, 4859, 3942, 3897, 3343, 4124, 4808, 2478, 839, 2523, 1119, 5171, 1938, 2790, 383, 5365, 1286, 2392, 2741, 3305, 1638, 2524, 90, 99, 1220, 3475, 1157, 2917, 1704, 2956, 4489, 5780, 3798, 4806, 4082, 1358, 4358, 3944, 2556, 4772, 1101, 290, 1699, 3667, 2944, 3326, 4414, 3862, 5752, 5294, 954, 2490, 3869, 3537, 5158, 3057, 706, 1379, 488, 5666, 4425, 983, 1297, 2200, 3065, 2817, 1015, 4455, 724, 5337, 3768, 5743, 1234, 1394, 2410, 1792, 2167, 5156, 4244, 4135, 3522, 2655, 1105, 5589, 5453, 1382, 1884, 4197, 3803, 1323, 1036, 5783, 591, 4394, 820, 22, 1709, 730, 4700, 260, 4249, 4734, 1325, 565, 5333, 2296, 1613, 3374, 1797, 3131, 3703, 4680, 3312, 5331, 5734, 2886, 4814, 3125, 1895, 5604, 127, 2445, 2435, 5172, 1360, 5417, 5715, 2302, 350, 37, 2163, 5626, 5181, 1487, 539, 5646, 3283, 182, 342, 4514, 3220, 790, 5487, 4060, 2515, 4381, 3865, 472, 3287, 829, 4551, 2017, 4503, 4030, 2055, 1566, 2117, 5098, 4439, 759, 5253, 5662, 997, 131, 5572, 2638, 5341, 406, 2818, 1055, 5647, 4311, 3456, 4515, 3797, 149, 3628, 2692, 5378, 1883, 5082, 5367, 3037, 4707, 2666, 227, 4715, 4118, 2121, 2314, 2058, 5667, 1822, 1834, 5242, 4791, 5621, 1070, 1994, 609, 4025, 381, 3607, 5657, 4108, 818, 684, 1309, 590, 2519, 1145, 2060, 2539, 2575, 449, 3362, 151, 4013, 2173, 3300, 616, 1279, 1911, 4179, 1025, 1832, 230, 252, 2319, 2184, 1283, 4384, 3202, 4944, 1662, 2154, 1153, 407, 4229, 3171, 1226, 1108, 5003, 3270, 1267, 3525, 703, 2693, 4988, 3767, 2540, 1563, 3882, 1766, 762, 710, 2763, 2210, 1818, 3584, 4961, 4688, 2397, 1304, 2799, 75, 732, 3683, 2197, 2629, 5020, 3876, 1380, 4845, 2750, 555, 853, 4620, 815, 2845, 2371, 2604, 4191, 5119, 1113, 3391, 2235, 3720, 2731, 2949, 328, 957, 670, 1041, 2446, 1976, 1320, 4843, 5386, 2735, 2221, 3432, 3591, 3196, 3396, 3585, 1887, 4298, 466, 2336, 1092, 4029, 3879, 974, 691, 3915, 755, 4130, 580, 5239, 1459, 832, 3937, 5700, 2576, 3308, 4653, 3280, 4963, 5207, 540, 373, 83, 905, 4530, 4279, 3620, 567, 1665, 3345, 2082, 795, 5075, 162, 1723, 4580, 4638, 2038, 2773, 4701, 3293, 1650, 4074, 4132, 1276, 4011, 2931, 2938, 4900, 1592, 5404, 4566, 2027, 4406, 1973, 3173, 4216, 640, 111, 1824, 2578, 3550, 2974, 4449, 888, 4528, 53, 1091, 389, 262, 4719, 31, 5306, 3489, 3541, 499, 204, 7, 560, 3315, 699, 5513, 1244, 2726, 1648, 943, 326, 3021, 5694, 3669, 3659, 2546, 5203, 1836, 4245, 1628, 3138, 3228, 2219, 3328, 1697, 1611, 3840, 1215, 3480, 2884, 2789, 4399, 1511, 5721, 503, 5570, 1700, 317, 5592, 129, 5577, 5539, 1675, 3769, 2301, 26, 1155, 5775, 1400, 3960, 4810, 1483, 4317, 3153, 3824, 5220, 4516, 2882, 1078, 4450, 2881, 4209, 2713, 4492, 1974, 2438, 4058, 4763, 5192, 1943, 4068, 2620, 4241, 5339, 2369, 5366, 5256, 3438, 1731, 2209, 1571, 825, 3105, 571, 2295, 217, 4102, 370, 4908, 4840, 5390, 3195, 455, 3831, 1348, 2663, 4836, 3022, 1219, 2330, 936, 3922, 3210, 783, 1162, 4593, 5048, 135, 3594, 3873, 4971, 3896, 1983, 1423, 3031, 3337, 2677, 20, 5302, 3925, 4019, 948, 5756, 4750, 4674, 1096, 3331, 3417, 4225, 3255, 610, 2103, 1328, 865, 2804, 2977, 3568, 2998, 4065, 2257, 4925, 914, 1030, 3587, 269, 3001, 5262, 2337, 2811, 4375, 3476, 3699, 3758, 306, 2218, 2934, 1251, 3290, 1085, 3482, 2900, 2992, 3965, 4678, 1559, 107, 4863, 766, 4345, 5552, 4271, 3555, 3608, 343, 2805, 5764, 2228, 4422, 1771, 2268, 221, 4073, 76, 134, 2522, 600, 2686, 3772, 1452, 3049, 336, 1125, 4957, 4181, 4350, 2909, 48, 3165, 546, 771, 1610, 5116, 5068, 2825, 1605, 496, 1231, 1577, 3929, 4109, 5385, 4429, 5138, 4749, 5043, 1291, 3246, 1048, 3263, 4303, 3112, 1937, 445, 4619, 3376, 2381, 4645, 2069, 1493, 2947, 2138, 434, 3472, 2409, 2106, 855, 1840, 2878, 2216, 3593, 5081, 360, 5656, 2011, 4022, 3863, 2403, 1257, 2553, 5766, 4496, 3403, 2422, 2687, 4467, 2341, 5397, 4263, 1061, 1793, 298, 1857, 2124, 543, 3505, 2831, 5422, 2676, 1740, 3749, 662, 5409, 1547, 576, 3992, 4325, 4991, 3477, 2323, 919, 4553, 4884, 1136, 233, 4742, 1375, 1759, 397, 5063, 1130, 5345, 4777, 4974, 5615, 1476, 2669, 4659, 5467, 5288, 4992, 2876, 3864, 1202, 2223, 3657, 3276, 850, 1450, 1282, 3777, 1519, 2207, 5058, 2501, 955, 4876, 5057, 4781, 3971, 3054, 3680, 3294, 3549, 4248, 1881, 88, 4198, 1814, 5651, 687, 1752, 229, 5245, 813, 1764, 439, 628, 4702, 4517, 4605, 1579, 2615, 939, 3011, 1103, 4546, 4083, 1456, 2760, 2668, 968, 1327, 2362, 5618, 4047, 4511, 43, 4572, 2591, 1841, 3120, 4100, 4721, 3658, 1179, 5627, 1302, 2354, 212, 1944, 1050, 1583, 4897, 3427, 3504, 3682, 666, 196, 2159, 2321, 2366, 3288, 906, 4989, 1551, 3740, 1732, 5588, 1350, 2018, 1860, 3495, 1657, 4247, 3074, 3069, 649, 3851, 961, 4313, 2542, 4654, 3269, 348, 3674, 4053, 4367, 928, 3866, 4147, 3250, 5568, 3724, 5001, 2213, 2969, 4935, 1629, 3580, 3676, 5553, 5489, 4421, 5531, 4563, 4146, 3557, 5544, 4407, 2262, 2681, 3332, 717, 1658, 3352, 5406, 1167, 146, 1787, 1419, 612, 2896, 4035, 798, 3577, 155, 4923, 5375, 3955, 2758, 1789, 897, 3801, 2954, 3671, 1942, 4289, 2225, 19, 57, 3624, 5372, 2770, 1280, 557, 2920, 1263, 3282, 780, 240, 2476, 2046, 1929, 1453, 3887, 330, 5066, 1696, 2429, 2915, 4670, 3064, 2459, 3252, 1853, 2227, 5762, 1804, 5740, 255, 2454, 5307, 646, 2536, 1736, 18, 879, 1678, 2170, 1564, 1372, 300, 4526, 3042, 3361, 4679, 2353, 4586, 1190, 1735, 380, 893, 2709, 1342, 1188, 1354, 2774, 2300, 2950, 4003, 2910, 3492, 4139, 1416, 5184, 2971, 2224, 3826, 5236, 3535, 864, 1624, 1082, 2157, 3390, 3633, 4984, 1864, 2568, 2800, 4954, 4624, 2714, 3548, 1528, 4327, 4361, 3663, 5665, 5759, 238, 1866, 1821, 5459, 3809, 2144, 2899, 5103, 5613, 3765, 3717, 1248, 2431, 966, 2613, 3126, 274, 4155, 3741, 3702, 173, 2247, 1126, 1921, 3726, 2547, 5168, 942, 1146, 341, 1898, 3463, 3778, 4359, 987, 1802, 117, 588, 1986, 3045, 1458, 2457, 5761, 278, 2585, 5152, 1854, 5468, 1156, 1575, 2140, 3712, 4754, 5323, 4097, 2795, 760, 4690, 2951, 1714, 1166, 913, 4370, 4607, 2706, 627, 302, 2939, 2844, 890, 2725, 4983, 59, 115, 5206, 2063, 2605, 453, 756, 2913, 2682, 1010, 513, 524, 5748, 647, 3291, 1746, 851, 4520, 2471, 4426, 4268, 778, 2179, 5037, 4192, 2, 3047, 5491, 894, 5556, 1016, 5185, 3243, 831, 2728, 89, 1813, 1925, 3888, 3878, 1019, 3742, 5148, 181, 1470, 1457, 3848, 3842, 5061, 1160, 4865, 351, 3400, 1164, 4939, 1258, 4709, 891, 624, 2858, 3256, 100, 1536, 2039, 2084, 5354, 1425, 859, 2012, 263, 4990, 840, 1503, 3893, 1441, 1880, 4173, 4382, 3513, 2552, 3521, 1473, 2136, 925, 279, 258, 4014, 3545, 5403, 5120, 1461, 5472, 3262, 2675, 3512, 4273, 899, 4738, 3899, 1568, 2481, 5199, 2526, 1666, 3080, 4899, 122, 154, 773, 2288, 4175, 5049, 1443, 5357, 3540, 352, 5008, 4975, 3701, 3627, 4296, 5065, 3278, 2226, 446, 5614, 1368, 5728, 4778, 2702, 3058, 5428, 3070, 184, 2535, 2334, 5055, 5452, 498, 5135, 2722, 1970, 3383, 2204, 3161, 5622, 4088, 3213, 2092, 476, 138, 3469, 2527, 5485, 3764, 3227, 2633, 374, 5586, 210, 2294, 5240, 821, 4433, 3181, 5439, 1806, 4213, 5490, 1359, 2087, 199, 4059, 4487, 785, 709, 2343, 1949, 2415, 3706, 4, 1290, 102, 3430, 2120, 4736, 550, 4454, 1915, 4332, 1183, 723, 1294, 1873, 3170, 4599, 1875, 94, 3117, 5371, 2727, 4040, 4284, 2474, 4388, 5456, 3762, 5164, 1553, 3324, 2716, 796, 1646, 3496, 5252, 5742, 4780, 3560, 5287, 5382, 3325, 1708, 2010, 2894, 1161, 5016, 2469, 3700, 4771, 791, 1688, 520, 2612, 4891, 3508, 2932, 690, 4929, 4873, 3895, 5224, 4039, 1174, 3273, 3298, 4202, 4940, 2499, 92, 1978, 3993, 862, 3647, 4424, 5243, 197, 2903, 5263, 2660, 3542, 1184, 5518, 4722, 1396, 5541, 5042, 2367, 205, 3735, 334, 2797, 4377, 4578, 1867, 2274, 2201, 1357, 1371, 467, 5610, 3651, 4854, 3665, 1931, 4153, 5750, 4490, 2531, 5218, 534, 2840, 4868, 2485, 1608, 4257, 319, 5226, 4729, 416, 3101, 836, 3316, 1988, 4969, 1436, 5128, 2355, 4506, 1656, 1788, 5289, 1807, 3242, 2929, 5131, 5440, 802, 1517, 4711, 1467, 5115, 4125, 5437, 1702, 807, 5725, 5736, 5033, 1149, 4044, 447, 1784, 679, 1713, 2022, 5347, 3, 359, 3517, 4299, 2030, 2534, 3426, 5022, 5735, 3124, 3709, 3782, 1869, 1632, 4411, 5319, 1808, 3087, 3947, 5209, 748, 1468, 4276, 312, 1180, 5483, 764, 1083, 486, 2070, 1523, 4901, 4446, 1641, 1465, 3366, 4243, 2643, 5052, 4230, 5338, 2732, 4832, 3488, 1006, 2293, 1472, 629, 887, 1906, 2076, 3713, 3926, 170, 3678, 784, 384, 735, 1745, 1918, 639, 5265, 5379, 1054, 3398, 1144, 2695, 4967, 3375, 3339, 1879, 4801, 665, 283, 1934, 2864, 3672, 578, 2618, 501, 753, 308, 5733, 4051, 4320, 5291, 4596, 1809, 5506, 124, 1486, 21, 3164, 4608, 1967, 2683, 1541, 4285, 5238, 1393, 2250, 2171, 1079, 4941, 2610, 2518, 4233, 17, 2711, 3583, 3467, 2005, 1640, 2320, 4093, 3318, 2461, 5056, 841, 4741, 3804, 4665, 1980, 5658, 838, 1135, 5671, 4703, 3215, 80, 4720, 4934, 3079, 3078, 3689, 4872, 4597, 1707, 1205, 4231, 5317, 5264, 5399, 3455, 2705, 1150, 3322, 2654, 1811, 5388, 648, 4111, 1616, 734, 4295, 621, 139, 1406, 1109, 3075, 2299, 1722, 1060, 4725, 4805, 5631, 215, 827, 1562, 1630, 3999, 5660, 1595, 411, 1413, 3368, 4435, 1398, 3039, 2924, 642, 2161, 1026, 4953, 494, 4222, 3830, 3614, 4970, 3346, 2373, 292, 1690, 1538, 5551, 3843, 1246, 2584, 1237, 4910, 4746, 5458, 4930, 1228, 324, 265, 358, 3673, 1311, 3590, 3527, 3382, 5599, 2455, 2316, 4308, 3589, 375, 668, 386, 4905, 1509, 566, 4319, 355, 2208, 2452, 544, 4916, 5113, 519, 364, 394, 1152, 655, 3650, 275, 147, 5505, 5364, 4685, 4794, 1351, 4540, 2041, 3704, 5223, 2191, 1617, 5769, 3904, 187, 3793, 3695, 3994, 3569, 3106, 5635, 1966, 4866, 2762, 5616, 5159, 3381, 396, 2905, 5129, 3425, 1725, 505, 2965, 2838, 776, 5355, 4626, 418, 3277, 379, 158, 4495, 3685, 667, 5175, 3727, 5077, 4723, 437, 5132, 977, 1635, 999, 1987, 3786, 4793, 2034, 3599, 249, 164, 4034, 2566, 526, 3911, 4348, 3179, 5145, 3306, 4301, 4444, 4159, 5332, 1122, 5073, 2453, 2229, 1009, 376, 5691, 1391, 3574, 3536, 5074, 2256, 3311, 5096, 2021, 1827, 1421, 4518, 5026, 4896, 660, 4543, 1602, 672, 5412, 2927, 5109, 4756, 3770, 5593, 4573, 1762, 650, 5325, 1965, 1794, 1059, 3559, 110, 2779, 2766, 1128, 867, 3145, 5623, 573, 4416, 3029, 402, 4996, 3631, 1052, 5309, 4156, 281, 2737, 3734, 5376, 3423, 2866, 2147, 2816, 484, 2468, 2904, 4027, 3889, 329, 4924, 722, 1855, 4862, 618, 3341, 779, 2630, 4033, 4928, 2254, 432, 3529, 4710, 3102, 3841, 4344, 5469, 5377, 2024, 5092, 1791, 5644, 5674, 4571, 4959, 5086, 3898, 1992, 3234, 3681, 438, 1844, 4484, 3239, 5187, 1087, 4140, 4228, 3097, 3344, 1387, 4510, 1003, 4631, 775, 4942, 5779, 3796, 160, 4997, 1516, 1225, 5540, 3910, 1469, 5739, 4076, 3941, 581, 4569, 801, 4964, 5383, 2617, 803, 1823, 561, 2308, 2854, 1524, 398, 2172, 3710, 3238, 3023, 4648, 460, 241, 1168, 4504, 3118, 1038, 309, 2383, 2922, 4466, 949, 1481, 4718, 4912, 2979, 4094, 1272, 4634, 1951, 4269, 4337, 186, 1420, 4314, 1332, 5677, 909, 4005, 3422, 1680, 2365, 1134, 3364, 1386, 5327, 5126, 4789, 3506, 4649, 1946, 1343, 1138, 4671, 1897, 682, 4024, 3003, 4591, 5124, 4867, 159, 5179, 4189, 1760, 3868, 3062, 3116, 5478, 3413, 461, 3354, 2056, 2493, 3913, 4478, 195, 321, 5695, 5509, 2808, 3399, 632, 3815, 1692, 736, 1919, 5731, 5720, 3115, 2267, 1439, 1828, 5067, 1902, 2967, 2978, 462, 222, 3357, 2496, 2860, 920, 1597, 4342, 1848, 4084, 1557, 3903, 4618, 823, 2317, 3806, 4116, 5724, 3951, 4122, 3602, 3610, 4239, 3956, 2696, 3275, 3618, 3811, 5100, 2796, 846, 4215, 5110, 2375, 4792, 68, 931, 4816, 1253, 3629, 1209, 1488, 3820, 2251, 5450, 4656, 1163, 270, 852, 3359, 3159, 1177, 2715, 1527, 4787, 3649, 3309, 2054, 1075, 1882, 2382, 5554, 3855, 2175, 3660, 1427, 11, 1011, 5683, 3954, 3737, 4622, 4886, 1751, 3694, 357, 10, 3265, 2419, 3812, 2943, 5466, 4513, 3570, 1770, 4069, 4686, 2586, 4430, 4560, 4166, 2238, 2516, 5664, 4154, 1878, 4889, 4362, 4187, 4585, 2310, 3802, 5470, 73, 4822, 5765, 5004, 1689, 5088, 3025, 3233, 3349, 3182, 711, 1550, 3677, 4278, 3114, 1871, 4326, 2037, 1705, 1862, 4219, 3348, 261, 1403, 1955, 3335, 1200, 2623, 3655, 4911, 1529, 630, 200, 3510, 5266, 2820, 4041, 485, 202, 4613, 3648, 489, 637, 3258, 4464, 3785, 515, 5445, 1593, 2822, 4497, 2962, 5177, 5575, 990, 2141, 2051, 5346, 4898, 5029, 3231, 3378, 797, 4915, 770, 5732, 3523, 3606, 5702, 3845, 4131, 5017, 3424, 5745, 521, 3071, 3622, 5271, 4576, 220, 1369, 4096, 3394, 2477, 625, 2912, 2756, 5538, 3232, 1066, 4391, 1567, 1982, 2023, 4646, 5324, 5358, 4978, 2717, 5059, 2573, 3094, 5567, 5045, 2707, 1033, 1466, 1799, 4949, 5722, 1389, 4477, 4408, 2105, 3988, 318, 4828, 1013, 847, 5157, 1346, 3474, 1274, 575, 532, 3558, 2636, 656, 1308, 2116, 4559, 5007, 4000, 2052, 2402, 285, 2279, 2315, 1256, 4352, 3068, 725, 3883, 3861, 4538, 415, 1654, 1277, 299, 548, 3000, 1307, 4329, 4874, 5030, 5362, 2260, 1520, 118, 3380, 50, 2035, 2108, 126, 598, 5044, 1960, 3415, 4004, 2479, 2994, 676, 2255, 923, 2036, 2492, 1363, 128, 4558, 2941, 5663, 2650, 425, 5713, 1089, 5619, 5205, 1210, 1826, 2448, 388, 4519, 3499, 3852, 932, 5555, 2983, 716, 4628, 2670, 1412, 3151, 3875, 8, 5125, 5716, 1507, 4419, 4305, 1839, 4606, 412, 3483, 2744, 1489, 3188, 340, 209, 5755, 2130, 1341, 4883, 757, 552, 444, 5139, 3395, 3235, 3729, 5280, 4288, 2427, 2533, 1008, 3837, 176, 4831, 2765, 441, 3486, 4369, 1211, 3046, 597, 4602, 2326, 3260, 4823, 929, 1243, 2114, 3086, 1888, 876, 5629, 2863, 508, 3553, 3792, 3533, 2347, 5774, 596, 880, 3748, 3166, 3175, 562, 2592, 641, 569, 5163, 1223, 3100, 2537, 5723, 1670, 87, 120, 536, 3055, 5400, 817, 289, 4045, 3141, 5227, 1872, 2587, 335, 2775, 4556, 4224, 4436, 2230, 4253, 1633, 5034, 4815, 3635, 1800, 4432, 4877, 4973, 4536, 5767, 1254, 952, 3267, 3093, 2812, 4206, 5474, 5632, 2025, 1319, 3222, 5208, 1404, 1187, 2258, 4385, 917, 4063, 2640, 1999, 901, 3137, 4943, 3034, 3206, 2502, 605, 510, 551, 3736, 2532, 2902, 1961, 1686, 2554, 3546, 5053, 3397, 4413, 2104, 2192, 1232, 4774, 2859, 4757, 2724, 3987, 3027, 2285, 3162, 311, 3920, 3408, 5447, 3808, 1681, 2002, 5454, 4681, 4031, 2339, 1352, 2769, 579, 3518, 3532, 1399, 4151, 834, 4038, 869, 3353, 313, 781, 5410, 4079, 1402, 3551, 2443, 2379, 989, 2329, 2008, 3609, 307, 105, 2596, 3611, 4463, 4786, 3401, 2483, 237, 3989, 556, 5591, 5284, 3670, 1495, 5368, 1695, 5507, 504, 3605, 553, 2853, 5272, 777, 5322, 4090, 1531, 2345, 2803, 422, 4509, 4331, 2957, 2188, 1513, 4057, 5502, 911, 4730, 1124, 2861, 2759, 1173, 1969, 4632, 2946, 1373, 3731, 4336, 1116, 2734, 3795, 2525, 2424, 1001, 3668, 5204, 5352, 3932, 3123, 2033, 2614, 2462, 1669, 3146, 4335, 3067, 3711, 3653, 1344, 3810, 1432, 5304, 3586, 4921, 3978, 793, 1227, 4099, 3909, 3572, 3286, 1024, 5093, 1685, 3098, 3975, 3827, 1285, 4364, 1313, 1374, 2065, 2305, 4266, 2388, 5729, 2356, 3813, 2303, 71, 1504, 1972, 282, 41, 2898, 1339, 4764, 2880, 3032, 4255, 3367, 4587, 3600, 3705, 3563, 3167, 5150, 1549, 1490, 287, 4630, 1480, 2434, 1499, 1336, 700, 918, 4987, 1287, 2073, 391, 288, 4250, 2541, 5680, 1430, 1442, 5681, 4979, 2745, 5196, 3187, 1207, 2771, 3224, 2332, 404, 5213, 3696, 4212, 2778, 2622, 4895, 3014, 2590, 1508, 5609, 2166, 3847, 2139, 5212, 1537, 2981, 2174, 601, 1673, 2963, 2110, 1570, 3752, 3571, 28, 3442, 5462, 3693, 1208, 483, 4071, 1851, 104, 4420, 2158, 2988, 881, 2198, 2335, 707, 1599, 4775, 268, 5301, 2031, 4636, 1991, 3446, 3168, 1558, 3084, 4588, 1143, 5005, 996, 1331, 2239, 1230, 2987, 3746, 2291, 4820, 2565, 811, 3948, 85, 4334, 471, 2100, 4668, 902, 4784, 5369, 3788, 3725, 3661, 2401, 4914, 423, 4324, 427, 3301, 1990, 794, 5225, 528, 3389, 4500, 2781, 2014, 1623, 5595, 5398, 1749, 3733, 4627, 4743, 4075, 1349, 1694, 2751, 2561, 4423, 1198, 2361, 3979, 1316, 2324, 1664, 1293, 3544, 3807, 1449, 535, 726, 3997, 4751, 4328, 2194, 177, 4666, 2122, 1977, 5493, 1099, 1295, 1526, 3833, 2099, 5684, 2127, 5535, 1235, 5770, 2836, 5520, 3759, 2787, 5421, 2879, 2828, 3630, 2364, 3982, 4769, 2964, 2217, 804, 1213, 1479, 1703, 2072, 4318, 1596, 3892, 878, 1719, 5241, 5718, 3197, 3184, 4879, 3053, 322, 1497, 254, 4188, 4389, 2551, 2856, 2196, 5515, 971, 4163, 5023, 872, 3531, 5757, 1365, 4623, 3516, 4838, 2935, 5014, 3461, 4847, 1998, 203, 2231, 2444, 3871, 5257, 749, 482, 2862, 4261, 587, 3109, 4753, 1266, 3799, 2482, 235, 2094, 165, 272, 5688, 1047, 1271, 4625, 2125, 1064, 4871, 3588, 2374, 1555, 2412, 2936, 1239, 4246, 1429, 2465, 1424, 3428, 1273, 1831, 3133, 3048, 3108, 5525, 1819, 5330, 2079, 2809, 1676, 3839, 421, 3462, 2340, 4673, 3479, 2720, 1580, 58, 4635, 5549, 5496, 244, 4360, 148, 4208, 5608, 3933, 5605, 4547, 5104, 661, 4234, 1560, 1979, 2739, 3018, 3484, 5112, 3043, 692, 4126, 4922, 5772, 531, 4966, 782, 3582, 4431, 1594, 2088, 5282, 95, 5576, 4274, 3718, 3119, 4650, 4252, 2634, 171, 4825, 4127, 145, 3771, 5117, 3497, 3083, 2823, 1532, 4855, 1615, 4353, 3028, 3597, 1098, 5267, 2042, 2653, 4698, 3416, 1842, 136, 413, 277, 3481, 1698, 443, 3061, 1772, 4178, 3977, 844, 3295, 4479, 1334, 727, 2593, 3132, 2784, 2626, 3509, 1395, 2595, 4158, 3082, 4227, 5146, 845, 2169, 2754, 1805, 23, 2346, 4006, 5477, 3225, 4374, 4579, 3302, 2598, 697, 926, 1962, 363, 5270, 1612, 2890, 1604, 4790, 3178, 2579, 1734, 2843, 2495, 2404, 3449, 4378, 3371, 1843, 4693, 809, 2380, 4848, 5028, 3450, 5495, 1090, 2439, 2463, 1292, 4577, 3872, 4544, 677, 3443, 1409, 3077, 228, 2387, 5384, 1525, 5625, 2918, 4611, 5230, 3241, 1781, 1384, 294, 2975, 558, 5699, 4143, 1540, 2211, 1056, 4104, 4919, 882, 4286, 584, 5214, 4552, 4662, 967, 2015, 5085, 4218, 346, 5737, 1269, 2243, 4952, 1314, 5569, 429, 4761, 3756, 293, 1367, 1912, 2450, 9, 4637, 594, 1837, 4533, 2107, 3050, 3174, 2395, 2489, 3708, 4297, 2342, 4023, 758, 4931, 4932, 1072, 5416, 604, 4704, 4107, 5408, 2729, 4534, 123, 2807, 1825, 5497, 5084, 2240, 4617, 3281, 2538, 3240, 5234, 2680, 1335, 2271, 3881, 2118, 2885, 1370, 1417, 2111, 1748, 3751, 889, 1440, 481, 399, 1444, 3033, 1582, 3908, 1007, 622, 1193, 3598, 554, 5011, 133, 232, 714, 4562, 2222, 150, 2164, 4981, 2780, 4085, 4570, 1621, 3850, 1500, 5374, 3429, 5229, 5099, 3880, 119, 5738, 3780, 2236, 56, 842, 2948, 613, 3519, 4869, 2234, 3009, 4265, 5394, 2350, 3800, 3327, 2577, 1554, 1733, 5118, 978, 3561, 2449, 2801, 2916, 1189, 674, 2645, 1110, 4321, 2829, 1491, 3313, 5211, 769, 5643, 5246, 2602, 4401, 419, 4052, 5166, 1737, 440, 4696, 2911, 4054, 3890, 259, 5051, 3794, 2131, 3340, 4782, 46, 2562, 509, 2747, 25, 2631, 5560, 3076, 4640, 3675, 2814, 3254, 1886, 5353, 3226, 4061, 3641, 5583, 2583, 5141, 4339, 3885, 4568, 4465, 5574, 81, 1535, 2659, 3406, 5532, 3884, 2581, 2868, 1217, 5144, 5070, 5607, 2543, 512, 4590, 607, 1778, 2093, 4103, 2484, 631, 2895, 459, 998, 3485, 5269, 1464, 3314, 658, 1236, 2001, 4933, 4505, 2730, 1820, 922, 5654, 1169, 3625, 593, 1262, 4112, 3575, 5746, 3714, 5202, 3020, 2991, 5015, 4783, 1129, 574, 2177, 2700, 2792, 5672, 2405, 2832, 3150, 3350, 1377, 2673, 2411, 1175, 3441, 1849, 3716, 4724, 3266, 3143, 2040, 4221, 323, 347, 3458, 356, 3538, 620, 3420, 1573, 36, 1620, 3134, 4405, 5012, 599, 3007, 3719, 1278, 4397, 4512, 3805, 898, 686, 2907, 1954, 5594, 2029, 5559, 2652, 267, 5046, 4162, 4440, 2869, 5617, 4165, 1661, 1885, 4542, 1431, 5754, 4256, 5480, 2982, 1139, 1758, 4584, 1971, 5558, 2521, 5655, 1534, 130, 1603, 787, 3776, 2635, 4948, 2372, 5313, 69, 1932, 3113, 1192, 1798, 3902, 1381, 3317, 2083, 3384, 3026, 2608, 5359, 493, 738, 4409, 3431, 3534, 4462, 5024, 5528, 3152, 1561, 1186, 2921, 2498, 2430, 4575, 2393, 4945, 4739, 2009, 3024, 1077, 5685, 4258, 1765, 1434, 5232, 4333, 1151, 2855, 2183, 4535, 5389, 2529, 4349, 4773, 1069, 2328, 3940, 1435, 2143, 886, 4070, 765, 2679, 924, 2283, 3906, 1093, 4894, 378, 4441, 66, 5429, 4812, 2995, 910, 4309, 2848, 4811, 1569, 1035, 525, 3775, 5169, 3040, 3191, 5194, 5349, 5038, 2148, 1437, 4294, 190, 4799, 3981, 740, 4892, 2244, 4160, 3961, 2609, 4846, 2648, 2281, 2000, 980, 2690, 3957, 3063, 2432, 1598, 239, 4036, 450, 2075, 4410, 1159, 2187, 3176, 953, 1776, 4418, 93, 314, 1835, 5025, 1684, 1104, 5545, 3662, 430, 2875, 2053, 4144, 4565, 743, 3052, 843, 945, 1768, 3526, 331, 2368, 4748, 5686, 3194, 3091, 3821, 1028, 3745, 2599, 3637, 1408, 1591, 1672, 4456, 1353, 5571, 3072, 938, 2569, 4676, 344, 51, 2632, 2597, 713, 4194, 2080, 5600, 4920, 2883, 5002, 5149, 903, 1753, 5488, 1040, 490, 2555, 1545, 1221, 2672, 2156, 5712, 608, 1388, 3015, 3307, 4947, 2089, 1020, 3459, 4150, 3004, 3466, 2370, 2996, 2970, 2358, 1275, 2520, 2511, 3502, 623, 4765, 3157, 417, 3454, 4499, 3347, 2783, 1649, 5711, 5636, 991, 4691, 2331, 3564, 2067, 1385, 1645, 4043, 5176, 4106, 678, 1058, 5543, 4818, 4968, 5041, 4136, 3180, 2085, 4137, 5311, 5310, 1896, 1763, 4549, 3219, 4443, 1634, 4095, 506, 3088, 1755, 3434, 339, 2016, 4841, 4842, 5137, 935, 2509, 5562, 4834, 5637, 5547, 49, 2270, 5510, 1901, 1171, 934, 3964, 1622, 264, 549, 1505, 976, 4238, 91, 5481, 5182, 3995, 2377, 4557, 1018, 4523, 4120, 5095, 941, 5645, 1679, 5237, 1121, 950, 5564, 816, 2351, 1415, 1074, 4240, 1191, 2115, 3996, 5123, 5534, 4302, 863, 469, 828, 4501, 3645, 3218, 4998, 3578, 3284, 1118, 3774, 3939, 1876, 1268, 746, 4821, 2488, 3656, 3753, 3249, 4290, 4849, 1005, 1114, 933, 2049, 1260, 2567, 4341, 4272, 4796, 1706, 930, 1750, 4759, 2286, 4927, 541, 251, 635, 2872, 77, 5606, 5482, 4717, 1240, 5707, 2930, 5318, 3901, 2560, 883, 5494, 1063, 4458, 1652, 2406, 4282, 3970, 2628, 2004, 4474, 172, 4283, 5297, 487, 5402, 3128, 2767, 2892, 2793, 3464, 3763, 5250, 276, 4870, 3818, 5441, 3136, 1539, 3984, 5573, 3329, 5590, 1936, 3990, 32, 4434, 5290, 4207, 664, 3825, 4171, 4123, 4616, 5233, 3447, 1858, 2685, 3592, 2611, 1935, 5198, 201, 4113, 611, 4852, 3638, 1324, 5180, 2694, 602, 3612, 1094, 5134, 3543, 1478, 4129, 144, 266, 4205, 4641, 436, 589, 2475, 5097, 1062, 5381, 1185, 1, 5343, 4788, 3856, 2497, 5274, 1298, 870, 3205, 169, 194, 1463, 1730, 3190, 4658, 4267, 2637, 1639, 4480, 3854, 3471, 2349, 1222, 2678, 4744, 1984, 3209, 226, 4026, 3142, 3121, 4438, 2276, 5580, 63, 4400, 2043, 3264, 4712, 5537, 4633, 5121, 2313, 327, 5035, 4235, 3524, 5557, 5500, 896, 5217, 2297, 4598, 3781, 4802, 4716, 3819, 2733, 3404, 1312, 5215, 1785, 2835, 4639, 2926, 2275, 1250, 3377, 433, 1418, 5401, 1642, 636, 1644, 2955, 4713, 3122, 685, 2937, 4878, 5542, 2772, 4882, 3928, 529, 4398, 3081, 1447, 5231, 3520, 617, 223, 47, 4907, 2137, 2464, 986, 993, 2086, 4330, 5249, 3636, 5107, 5342, 4524, 2698, 4610, 296, 5299, 3393, 1043, 4304, 2999, 3715, 4903, 2460, 4993, 1255, 5387, 2264, 767, 1195, 659, 681, 2394, 4657, 5498, 4830, 4373, 3514, 4002, 5425, 5633, 3236, 3296, 4937, 4172, 2887, 2558, 1318, 619, 547, 4651, 4728, 2199, 704, 5443, 1845, 4615, 2385, 4242, 2081, 2363, 3200, 3985, 2151, 4643, 3547, 5581, 3601, 1552, 2050, 2414, 4380, 874, 1034, 5778, 236, 2942, 474, 1362, 1720, 5279, 3019, 3351, 4371, 5414, 295, 1769, 533, 4199, 5127, 1242, 2743, 5186, 3342, 5361, 4448, 193, 3229, 1767, 3969, 2752, 741, 475, 4138, 4009, 4807, 4752, 2563, 4056, 2278, 2261, 2359, 962, 5216, 4705, 3373, 3247, 951, 4539, 5524, 5603, 3619, 4322, 1904, 3147, 1084, 789, 5492, 2071, 673, 1712, 2513, 5464, 218, 1474, 1022, 2451, 4733, 1817, 4582, 5563, 5350, 5561, 719, 1533, 2389, 141, 5670, 2186, 2090, 5040, 4201, 3721, 3687, 216, 349, 5396, 3135, 916, 1426, 1671, 915, 108, 3917, 24, 2307, 1677, 2986, 3457, 125, 4581, 1790, 2582, 1057, 42, 208, 856, 456, 5418, 4365, 5094, 819, 4357, 225, 1051, 1996, 3008, 3654, 4502, 3158, 2280, 452, 680, 3744, 1856, 5111, 1410, 2064, 468, 1281, 4152, 5516, 3099, 2846, 1726, 2662, 3732, 4214, 2237, 733, 3528, 435, 368, 338, 3912, 4195, 1049, 5248, 2664, 1546, 4157, 2742, 2132, 3149, 2545, 5434, 786, 5316, 4323, 153, 3686, 1655, 390, 1383, 606, 653, 2155, 3038, 1847, 2045, 5188, 1310, 1993, 689, 4962, 5079, 3204, 4412, 286, 3110, 4048, 3473, 5612, 5336, 982, 35, 5457, 5689, 4032, 4018, 3596, 3013, 3640, 5499, 5219, 2145, 470, 2259, 4839, 502, 1643, 5579, 5624, 3900, 3698, 2007, 1345, 242, 1717, 163, 2019, 2428, 4601, 3870, 4767, 5351, 82, 3595, 2442, 301, 1165, 2791, 409, 582, 4652, 1584, 824, 2486, 168, 401, 1586, 1940, 3439, 5197, 522, 1498, 4176, 592, 4168, 1952, 2352, 191, 4906, 5503, 234, 3998, 4677, 5348, 1158, 2190, 5142, 5069, 3127, 2059, 5461, 2776, 1071, 369, 4020, 3156, 5032, 1590, 4951, 3387, 2802, 3980, 1964, 2396, 1606, 3832, 1037, 273, 2441, 633, 473, 2026, 1711, 5087, 1245, 2487, 5703, 1581, 2413, 5039, 3697, 4644, 1301, 4174, 1501, 3691, 1411, 745, 4785, 3829, 614, 4600, 3728, 3664, 116, 792, 4858, 3261, 1908, 742, 4386, 257, 2149, 1322, 2306, 5244, 5258, 2078, 385, 4735, 3103, 2074, 2857, 5407, 1777, 2142, 3817, 1659, 2420, 2674, 3418, 1729, 2416, 2126, 2263, 3437, 985, 5021, 1364, 3409, 2877, 1326, 1572, 4193, 4186, 4861, 1958, 822, 4529, 2559, 2647, 2284, 1710, 3747, 5071, 157, 4476, 4844, 3365, 2968, 3877, 5334, 1017, 5442, 5089, 5705, 1667, 3203, 2253, 3035, 2980, 4541, 3936, 2777, 4708, 4372, 5393, 4185, 2113, 1021, 1270, 4682, 1229, 4390, 5533, 2269, 256, 1259, 5259, 2699, 3271, 4164, 4687, 5444, 4021, 3791, 2753, 1916, 1397, 1718, 4417, 4262, 5391, 3107, 5174, 5273, 1445, 2897, 3268, 4028, 3221, 3692, 3478, 1754, 1893, 426, 5373, 5602, 2976, 3679, 5446, 1945, 4001, 2344, 2557, 1201, 2847, 1795, 1668, 5747, 5160, 3379, 2096, 1660, 4795, 1774, 5356, 5031, 5753, 2606, 4909, 2185, 188, 4694, 4110, 1683, 4621, 5430, 3539, 2066, 4857, 114, 2644, 2564, 1724, 1838, 3435, 4483, 4525, 3639, 5436, 2671, 137, 4755, 1514, 3085, 5106, 4699, 572, 2989, 4776, 1957, 4404, 5296, 1401, 382, 5585, 60, 4077, 1471, 1927, 448, 4114, 1218, 3089, 3257, 132, 1877, 2128, 3766, 3567, 4300, 3285, 5673, 511, 1923, 1123, 315, 4145, 3617, 5010, 3642, 4574, 5511, 5432, 2168, 2755, 4913, 1510, 5620, 5776, 121, 1631, 5504, 4315, 4629, 2661, 912, 1252, 4982, 5706, 2908, 2408, 5566, 65, 537, 4387, 2480, 3923, 5741, 4346, 5411, 1941, 1905, 1042, 4148, 4851, 1812, 2873, 4254, 1428, 3838, 34, 4355, 708, 5080, 1950, 4196, 975, 5427, 4078, 192, 3652, 1224, 568, 1376, 2973, 3991, 3036, 4363, 5102, 2925, 4917, 1914, 2667, 1305, 1728, 5519, 1892, 2289, 810, 2874, 2102, 4885, 4595, 2426, 671, 206, 3310, 940, 1779, 3279, 2249, 2292, 1738, 5640, 4583, 3030, 1773, 2544, 2472, 2997, 219, 2794, 2621, 4938, 4860, 1095, 366, 12, 4824, 4089, 1995, 30, 98, 1194, 4683, 5009, 3468, 4183, 854, 1477, 3388, 2398, 2003, 559, 5162, 1863, 683, 799, 4481, 5200, 4853, 500, 729, 1141, 2933, 5758, 1206, 3419, 970, 3453, 5335, 1181, 3958, 367, 4522, 405, 5292, 13, 3274, 1556, 4827, 4226, 3407, 1454, 1405, 3304, 4475, 695, 5455, 2656, 3319, 2851, 3828, 3192, 1097, 3666, 5261, 3836, 4488, 491, 4956, 871, 2214, 3010, 3199, 5426, 362, 2077, 675, 5697, 2162, 3451, 3487, 3193, 3355, 5122, 1133, 2548, 3244, 1076, 4142, 3581, 4564, 211, 5749, 1012, 3755, 103, 1601, 1637, 2378, 250, 2057, 2824, 944, 2436, 5649, 4149, 4612, 5698, 2421, 2833, 3783, 1485, 4280, 2651, 1859, 583, 5091, 4066, 5285, 5696, 2400, 5210, 4609, 3493, 4368, 5268, 2688, 1414, 5147, 2867, 3392, 2298, 1956, 1029, 2503, 4190, 4706, 4647, 5708, 1247, 15, 4316, 643, 1261, 4469, 1046, 84, 377, 4555, 5476, 3953, 4427, 2718, 1068, 304, 4293, 2386, 38, 4664, 2333, 3722, 3983, 3498, 4999, 2304, 5247, 5521, 1182, 2815, 538, 2202, 1120, 1889, 4292, 626, 5546, 2182, 2738, 4037, 2417, 995, 4995, 1300, 2091, 3177, 1355, 3924, 2768, 183, 5777, 161, 2985, 921, 72, 1306, 1111, 2850, 5760, 1739, 5154, 527, 2549, 2399, 2466, 964, 1317, 1691, 4955, 3186, 387, 1446, 3943, 3494, 1506, 3259, 253, 5727, 345, 1515, 693, 70, 3950, 5687, 1574, 109, 1023, 3092, 1004, 4428, 663, 4383, 3292, 2242, 4101, 61, 3465, 2062, 4223, 585, 1600, 5475, 2906, 4379, 5471, 5260, 4180, 1747, 3918, 718, 2512, 1618, 1907, 372, 4182, 1651, 4663, 4121, 833, 3198, 5360, 4220, 3130, 3730, 696, 3096, 5178, 2252, 3760, 2505, 2044, 5170, 1928, 1865, 4548, 4918, 2665, 3111, 4697, 2407, 877, 1782, 2764, 563, 451, 1588, 2993, 4809, 4521, 3976, 4972, 5136, 3501, 2952, 1100, 835, 1981, 1460, 1199, 3779, 4486, 1233, 1214, 2470, 3626, 1625, 175, 1653, 2798, 4800, 5062, 3005, 4669, 1924, 1786, 3952, 747, 337, 3750, 2418, 1475, 4338, 2098, 1172, 654, 1870, 5773, 3411, 303, 2391, 5298, 2821, 4695, 4545, 2600, 4980, 4684, 2891, 96, 5659, 1073, 3163, 3857, 4655, 4813, 4134, 3491, 74, 3930, 2570, 5781, 1663, 291, 3916, 2528, 774, 1741, 5370, 5076, 5424, 3867, 2195, 4473, 4287, 3155, 1780, 3934, 27, 284, 5717, 5, 5744, 142, 5153, 4050, 5433, 5634, 3603, 4403, 178, 754, 3503, 2246, 5730, 5771, 5751, 4167, 739, 1140, 4200, 4461, 857, 3440, 1687, 1117, 1448, 1775, 1609, 5628, 1044, 701, 3853, 3154, 354, 849, 965, 5587, 3972, 3320, 3530, 410, 1939, 5782, 972, 884, 1810, 1565, 5190, 3336, 1264, 4976, 2870, 1031, 2006, 3211, 1482, 3095, 1106, 644, 2550, 994, 752, 458, 3212, 2849, 3433, 2277, 4366, 5714, 3739, 4604, 3386, 4768, 1249, 5517, 3835, 5473, 947, 2517, 2625, 5235, 1053, 2721, 5315, 1933, 3436, 2748, 5305, 4437, 454, 2757, 2153, 180, 1265, 3849, 4343, 2508, 2433, 4008, 4667, 4779, 495, 5019, 3414, 2691, 2928, 3874, 1088, 247, 3297, 271, 400, 3507, 2701, 1045, 3707, 5173, 5479, 3017, 1715, 1796, 3757, 2901, 1930, 721, 5526, 3289, 40, 3743, 5295, 245, 3334, 2504, 3579, 5768, 1299, 1900, 4091, 79, 4306, 3973, 4747, 1743, 4714, 2914, 2233, 4010, 3129, 166, 634, 4237, 2953, 4758, 428, 3646, 3935, 4170, 5060, 2940, 1131, 586, 1903, 3566, 101, 2723, 3959, 2013, 3402, 3183, 3251, 371, 1997, 395, 4080, 4554, 542, 4210, 2357, 805, 2984, 4351, 44, 4067, 3370, 2215, 5642, 2945, 1330, 895, 5278, 963, 3967, 4062, 310, 1846, 4494, 4275, 1627, 156, 1926, 1607, 5133, 4537, 2458, 750, 1744, 812, 4259, 5536, 2819, 1899, 4893, 2266, 4594, 112, 2708, 2990, 1296, 2134, 4532, 1107, 5596, 3962, 5277, 2689, 1366, 5286, 5527, 4660, 1783, 2810, 185, 5423, 4732, 638, 694, 5448, 1080, 1203, 4798, 3834, 5512, 5329, 1502, 5449, 984, 4819, 3090, 4453, 113, 3073, 2028, 14, 1874, 2206, 1890, 4046, 5314, 860, 2181, 2135, 174, 1868, 5191, 4307, 5630, 1512, 4731, 4177, 1127, 1002, 763, 5679, 1761, 4055, 2619, 4393, 595, 3051, 54, 62, 4310, 5275, 1727, 3515, 4770, 2788, 3303, 2827, 1148, 2133, 1518, 5661, 5486, 2871, 4340, 959, 4727, 761, 106, 4507, 5601, 4264, 1014, 3886, 2785, 332, 2119, 3189, 1619, 5668, 737, 2888, 5293, 2972, 2712, 0, 1390, 5669, 3356, 2919, 4098, 2265, 2189, 4347, 5195, 1894, 214, 1852, 1455, 3986, 5143, 5719, 1636, 3632, 3844, 1626, 5548, 2068, 3056, 1850, 5438, 4642, 5611, 5653, 5320, 2376, 420, 1959, 3891, 5463, 3949, 2440, 246, 2646, 4459, 3160, 3738, 615, 6, 800, 5650, 224, 3104, 2097, 5726, 1065, 1170, 5578, 3372, 3208, 1757, 2425, 866, 5763, 4888, 1347, 4260, 4550, 2786, 1196, 4589, 4766, 2456, 3002, 4012, 4835, 2507, 4880, 4958, 2601, 55, 2589, 2966, 837, 231, 3223, 5435, 2580, 457, 868, 3860, 3333, 4105, 5682, 1238]\n",
            "Train Dataset: 2892\n",
            "Valid Dataset: 2892\n",
            "Test Dataset: 2893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qagxK3-38TQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle"
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHUjtXa22DN"
      },
      "source": [
        "net = alexnet() # Loading AlexNet model\n",
        "\n",
        "# AlexNet has 1000 output neurons, corresponding to the 1000 ImageNet's classes\n",
        "# We need 101 outputs for Caltech-101\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer\n",
        "                                                 # The convolutional layer is nn.Conv2d\n",
        "\n",
        "# We just changed the last layer of AlexNet with a new fully connected layer with 101 outputs\n",
        "# It is strongly suggested to study torchvision.models.alexnet source code"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc"
      },
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcoQ5fD49yT_",
        "outputId": "1cfac4ee-230c-46cd-e94c-723c2098ac69"
      },
      "source": [
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step() "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/50, LR = [0.01]\n",
            "Step 0, Loss 4.623413562774658\n",
            "Step 10, Loss 4.611185550689697\n",
            "Starting epoch 2/50, LR = [0.01]\n",
            "Step 20, Loss 4.5904035568237305\n",
            "Starting epoch 3/50, LR = [0.01]\n",
            "Step 30, Loss 4.564298629760742\n",
            "Starting epoch 4/50, LR = [0.01]\n",
            "Step 40, Loss 4.509694576263428\n",
            "Starting epoch 5/50, LR = [0.01]\n",
            "Step 50, Loss 4.481357097625732\n",
            "Starting epoch 6/50, LR = [0.01]\n",
            "Step 60, Loss 4.3375372886657715\n",
            "Starting epoch 7/50, LR = [0.01]\n",
            "Step 70, Loss 4.141532897949219\n",
            "Starting epoch 8/50, LR = [0.01]\n",
            "Step 80, Loss 4.138078212738037\n",
            "Starting epoch 9/50, LR = [0.01]\n",
            "Step 90, Loss 4.221368312835693\n",
            "Starting epoch 10/50, LR = [0.01]\n",
            "Step 100, Loss 4.249381065368652\n",
            "Starting epoch 11/50, LR = [0.01]\n",
            "Step 110, Loss 4.1118364334106445\n",
            "Step 120, Loss 3.988600730895996\n",
            "Starting epoch 12/50, LR = [0.01]\n",
            "Step 130, Loss 3.964048147201538\n",
            "Starting epoch 13/50, LR = [0.01]\n",
            "Step 140, Loss 3.895540714263916\n",
            "Starting epoch 14/50, LR = [0.01]\n",
            "Step 150, Loss 3.8449621200561523\n",
            "Starting epoch 15/50, LR = [0.01]\n",
            "Step 160, Loss 3.401367425918579\n",
            "Starting epoch 16/50, LR = [0.01]\n",
            "Step 170, Loss 3.727207660675049\n",
            "Starting epoch 17/50, LR = [0.01]\n",
            "Step 180, Loss 3.502075433731079\n",
            "Starting epoch 18/50, LR = [0.01]\n",
            "Step 190, Loss 3.629641532897949\n",
            "Starting epoch 19/50, LR = [0.01]\n",
            "Step 200, Loss 3.1970386505126953\n",
            "Starting epoch 20/50, LR = [0.01]\n",
            "Step 210, Loss 3.2285048961639404\n",
            "Starting epoch 21/50, LR = [0.0001]\n",
            "Step 220, Loss 2.9241018295288086\n",
            "Step 230, Loss 3.046025276184082\n",
            "Starting epoch 22/50, LR = [0.001]\n",
            "Step 240, Loss 2.964628219604492\n",
            "Starting epoch 23/50, LR = [0.001]\n",
            "Step 250, Loss 3.009653329849243\n",
            "Starting epoch 24/50, LR = [0.001]\n",
            "Step 260, Loss 2.954749345779419\n",
            "Starting epoch 25/50, LR = [0.001]\n",
            "Step 270, Loss 2.8376829624176025\n",
            "Starting epoch 26/50, LR = [0.001]\n",
            "Step 280, Loss 2.855471611022949\n",
            "Starting epoch 27/50, LR = [0.001]\n",
            "Step 290, Loss 2.948621988296509\n",
            "Starting epoch 28/50, LR = [0.001]\n",
            "Step 300, Loss 2.7588930130004883\n",
            "Starting epoch 29/50, LR = [0.001]\n",
            "Step 310, Loss 2.9382848739624023\n",
            "Starting epoch 30/50, LR = [0.001]\n",
            "Step 320, Loss 2.936408281326294\n",
            "Starting epoch 31/50, LR = [0.001]\n",
            "Step 330, Loss 2.8309550285339355\n",
            "Step 340, Loss 2.797527313232422\n",
            "Starting epoch 32/50, LR = [0.001]\n",
            "Step 350, Loss 2.8982748985290527\n",
            "Starting epoch 33/50, LR = [0.001]\n",
            "Step 360, Loss 2.731156349182129\n",
            "Starting epoch 34/50, LR = [0.001]\n",
            "Step 370, Loss 2.9611594676971436\n",
            "Starting epoch 35/50, LR = [0.001]\n",
            "Step 380, Loss 2.7932286262512207\n",
            "Starting epoch 36/50, LR = [0.001]\n",
            "Step 390, Loss 2.7991597652435303\n",
            "Starting epoch 37/50, LR = [0.001]\n",
            "Step 400, Loss 2.8210222721099854\n",
            "Starting epoch 38/50, LR = [0.001]\n",
            "Step 410, Loss 2.7181148529052734\n",
            "Starting epoch 39/50, LR = [0.001]\n",
            "Step 420, Loss 2.6178536415100098\n",
            "Starting epoch 40/50, LR = [0.001]\n",
            "Step 430, Loss 2.6021034717559814\n",
            "Starting epoch 41/50, LR = [1e-05]\n",
            "Step 440, Loss 2.588374614715576\n",
            "Step 450, Loss 2.578770399093628\n",
            "Starting epoch 42/50, LR = [0.0001]\n",
            "Step 460, Loss 2.6085619926452637\n",
            "Starting epoch 43/50, LR = [0.0001]\n",
            "Step 470, Loss 2.5832674503326416\n",
            "Starting epoch 44/50, LR = [0.0001]\n",
            "Step 480, Loss 2.604969024658203\n",
            "Starting epoch 45/50, LR = [0.0001]\n",
            "Step 490, Loss 2.591743230819702\n",
            "Starting epoch 46/50, LR = [0.0001]\n",
            "Step 500, Loss 2.644279956817627\n",
            "Starting epoch 47/50, LR = [0.0001]\n",
            "Step 510, Loss 2.4895527362823486\n",
            "Starting epoch 48/50, LR = [0.0001]\n",
            "Step 520, Loss 2.78552508354187\n",
            "Starting epoch 49/50, LR = [0.0001]\n",
            "Step 530, Loss 2.76845383644104\n",
            "Starting epoch 50/50, LR = [0.0001]\n",
            "Step 540, Loss 2.7335896492004395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO3HV5pqJg1o",
        "outputId": "3570ad65-4397-42fb-d5b0-3b10fc8c6449"
      },
      "source": [
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(val_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(val_dataset))\n",
        "\n",
        "print('Validation Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 12/12 [00:07<00:00,  1.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.29564315352697096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxekmR745ySe"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSHcUqLB5yWO",
        "outputId": "942a1fc1-502f-4642-aa9a-405744012629"
      },
      "source": [
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 12/12 [00:07<00:00,  1.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.29934324230902176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1ydBDBlx2Wg"
      },
      "source": [
        "torch.save(net.state_dict(), \"model.pth\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIeakUjcyTnz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxn00mbCyPSs"
      },
      "source": [
        "# For freezeing some layers and retrain\n",
        "#for param in net.parameters():\n",
        "    #param.requires_grad = False\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcMCGGqj0TI1"
      },
      "source": [
        "# For data augmentation aggiungere trasformazioni"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXs4stAZp7PI"
      },
      "source": [
        "# Step 3 Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAzntejMqD2l",
        "outputId": "73a993d3-c8b0-4b17-9ac1-0fccaeb8dd86"
      },
      "source": [
        "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
        "AlexNet_model.eval()\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), \n",
        "                                      normalize # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                     normalize                                    \n",
        "])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7DOSvIeyTW9",
        "outputId": "d9a28b28-d17c-4c00-cf3c-c7a33d553ddb"
      },
      "source": [
        "print(AlexNet_model.eval())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMxrcLDgzbwf"
      },
      "source": [
        "for param in AlexNet_model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM2itXG2rja3",
        "outputId": "ced5b05a-1d07-48e9-ff79-459456ccaf4c"
      },
      "source": [
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = Caltech(DATA_DIR, split='train',  transform=train_transform)\n",
        "test_dataset = Caltech(DATA_DIR, split='test', transform=eval_transform)\n",
        "print(len(train_dataset))\n",
        "indexes = list(range(5784))\n",
        "indexes= random.sample(indexes, len(indexes))\n",
        "print(indexes)\n",
        "train_indexes = indexes[:int(len(indexes)/2)]# split the indices for your train split\n",
        "val_indexes = indexes[int(len(indexes)/2):]# split the indices for your val split\n",
        "\n",
        "\n",
        "\n",
        "val_dataset = Subset(train_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5784\n",
            "[5025, 3989, 4338, 2486, 4076, 1662, 1230, 5559, 3233, 46, 949, 5290, 1021, 308, 5062, 2300, 314, 3642, 2576, 5651, 2693, 4737, 1536, 3496, 5746, 5588, 5326, 3449, 5688, 4756, 4568, 3194, 1412, 3436, 1648, 4930, 1484, 96, 2750, 3983, 5221, 1817, 4369, 4680, 5120, 706, 479, 1983, 2846, 4589, 456, 1489, 694, 5706, 4437, 2003, 5077, 741, 1539, 472, 3317, 4323, 4968, 3729, 1173, 2216, 127, 2213, 5762, 4043, 287, 5402, 3718, 274, 5139, 762, 3318, 3235, 5151, 352, 1868, 4424, 759, 2662, 4738, 1415, 4385, 1731, 5173, 5427, 4034, 3272, 3752, 1055, 3714, 1944, 3331, 3289, 5629, 4592, 1857, 1390, 4335, 1756, 3924, 3435, 5412, 4371, 4772, 4691, 983, 3866, 1464, 5683, 1126, 3809, 4936, 3731, 1882, 688, 5121, 792, 129, 4479, 3748, 3094, 3473, 5314, 4610, 4759, 2829, 1564, 3677, 4070, 41, 2898, 1802, 2599, 4786, 2287, 4352, 3179, 3072, 1933, 3759, 2222, 1773, 2038, 93, 1795, 975, 3371, 4291, 5029, 1543, 1905, 323, 3140, 4284, 4455, 976, 3715, 2286, 4221, 61, 1195, 1531, 5748, 2514, 2974, 1371, 4135, 722, 4105, 3291, 1947, 1641, 4905, 4728, 555, 5115, 92, 4807, 608, 898, 2051, 5587, 3899, 5660, 2879, 3354, 1258, 5403, 450, 2407, 2087, 213, 2175, 891, 4035, 2078, 639, 1798, 1158, 5541, 4533, 2768, 1809, 2273, 2698, 1202, 2786, 2748, 4969, 5355, 3012, 70, 1560, 1169, 1194, 1005, 435, 509, 4766, 2159, 3338, 2668, 1409, 1015, 2870, 1542, 4078, 4197, 3210, 3659, 4683, 3537, 2360, 4504, 2250, 4769, 292, 1814, 657, 2777, 905, 5312, 2119, 5480, 1861, 5492, 3001, 4548, 2672, 2922, 938, 1822, 4375, 1166, 4277, 1919, 2331, 987, 2799, 5330, 3188, 4389, 2393, 5067, 4048, 3902, 3645, 2757, 5247, 3324, 296, 4550, 5104, 5744, 1327, 1923, 326, 410, 5691, 1959, 2053, 553, 468, 1217, 5424, 1661, 4304, 165, 4833, 4947, 3707, 2814, 3800, 2356, 773, 4781, 5114, 2090, 3273, 2139, 5070, 4202, 4153, 1514, 5300, 800, 212, 4090, 2671, 4358, 1289, 4177, 1486, 1341, 3964, 1625, 114, 55, 2929, 3173, 4761, 1520, 3653, 1201, 447, 1367, 5694, 5446, 5351, 4407, 3960, 2036, 4392, 3791, 43, 2111, 1907, 1420, 5214, 3232, 2304, 2127, 5028, 3828, 1704, 5168, 2725, 1988, 1556, 3453, 5243, 2685, 5187, 3237, 258, 1413, 5094, 1119, 2982, 2352, 3939, 5755, 4629, 823, 2615, 5362, 506, 3805, 2500, 2790, 2005, 3288, 5751, 1725, 103, 3921, 5346, 5277, 5514, 3396, 5631, 4569, 5106, 4108, 5349, 3694, 2487, 1238, 872, 4960, 3862, 5774, 2312, 5357, 961, 680, 1946, 507, 4672, 5031, 2716, 3940, 2415, 4524, 5123, 3607, 4943, 2589, 948, 222, 2162, 4668, 1775, 725, 5508, 98, 1226, 3243, 1179, 4637, 2034, 1326, 5118, 5636, 4344, 420, 2445, 2574, 1303, 1860, 1968, 3850, 4452, 429, 2143, 5423, 3070, 3514, 1138, 3551, 2233, 5705, 4763, 5605, 2133, 3741, 3055, 474, 2210, 3505, 5709, 2296, 1264, 1873, 1220, 81, 4246, 5659, 5308, 2702, 4634, 1394, 2351, 5473, 3935, 5568, 4232, 1984, 2073, 1039, 3051, 1200, 2591, 4009, 1576, 1825, 2556, 3690, 502, 4017, 1628, 2345, 2888, 313, 5726, 5767, 3887, 5334, 2102, 2138, 986, 2861, 4347, 3392, 1296, 685, 5324, 1856, 3724, 2336, 5775, 1793, 3616, 2573, 1846, 2426, 3987, 3765, 2747, 3203, 3043, 117, 2272, 254, 1565, 1219, 1431, 4655, 2452, 3299, 5240, 4998, 1745, 2472, 2791, 309, 284, 4506, 3576, 3328, 3045, 5671, 4966, 240, 2396, 5739, 4796, 2688, 1391, 4652, 3329, 1199, 3925, 4609, 4607, 5093, 1002, 3990, 4800, 2095, 2248, 3739, 1274, 418, 133, 2545, 3673, 2212, 3150, 5447, 2900, 2595, 936, 1441, 2238, 3395, 1570, 4056, 2706, 2713, 5271, 5299, 4194, 894, 2079, 674, 888, 2603, 0, 2579, 700, 3153, 4331, 5575, 3608, 5074, 4150, 2813, 3269, 594, 1759, 4720, 4488, 3821, 5084, 3071, 4390, 1229, 1826, 443, 942, 4091, 2293, 2533, 1493, 3306, 3057, 514, 345, 2609, 5158, 5220, 4850, 3511, 5202, 4802, 4961, 493, 2147, 588, 5495, 1282, 4619, 3679, 3061, 2639, 732, 4872, 4571, 1281, 1322, 980, 3817, 524, 2170, 2874, 2712, 1439, 548, 2382, 5249, 677, 1128, 584, 8, 5563, 5685, 3212, 977, 4117, 846, 4881, 5621, 5087, 718, 951, 2153, 5745, 4956, 3251, 3464, 3253, 4844, 4937, 2934, 4383, 2235, 4657, 3991, 5208, 1302, 1387, 5468, 480, 3806, 4287, 4535, 1215, 3824, 1013, 2123, 2093, 4679, 5727, 76, 3471, 2289, 269, 1161, 5506, 4811, 4085, 2049, 4278, 2164, 22, 4647, 2421, 4973, 167, 3620, 1182, 3252, 4971, 1714, 2560, 4898, 5169, 2558, 4254, 1151, 2477, 2607, 3588, 580, 1006, 1008, 2055, 5356, 511, 5352, 3657, 5262, 3388, 439, 2478, 1172, 1533, 5321, 2704, 1812, 1488, 1475, 4214, 1540, 3021, 23, 4908, 1829, 3160, 463, 383, 5053, 4754, 1135, 3456, 4456, 2468, 2612, 4982, 2200, 2543, 5003, 1335, 4199, 2544, 4398, 4508, 1651, 36, 2144, 2292, 5353, 407, 5504, 5301, 2724, 109, 2783, 4360, 3540, 1309, 4575, 3506, 2221, 100, 2519, 5448, 3986, 1904, 5111, 628, 4258, 2012, 1646, 5420, 3590, 140, 1280, 5743, 4430, 4301, 3853, 1884, 3961, 3886, 3285, 621, 806, 973, 263, 1474, 902, 2325, 1842, 2891, 4204, 5124, 5339, 3189, 2839, 1088, 4582, 4977, 3536, 4307, 227, 5259, 2732, 874, 2931, 1145, 2403, 4132, 3132, 3113, 1043, 1028, 3844, 1804, 229, 3350, 533, 4154, 1578, 3529, 3835, 4354, 3962, 1332, 1465, 3183, 5286, 4094, 3002, 4042, 5556, 5633, 5230, 3779, 5564, 215, 5095, 3706, 803, 884, 3719, 4889, 1418, 2803, 5571, 325, 290, 4628, 4312, 4874, 4067, 4045, 4403, 247, 3635, 177, 3304, 1115, 1840, 236, 3050, 3067, 1125, 2314, 246, 2864, 3462, 1107, 3937, 3397, 754, 1559, 1265, 3265, 1074, 5222, 2103, 205, 4631, 178, 4294, 3446, 1962, 2516, 2539, 1453, 4522, 4741, 2171, 702, 3202, 2035, 1698, 2494, 971, 2774, 3784, 4104, 4308, 2529, 1778, 4411, 5704, 1657, 2208, 3060, 1901, 4348, 5729, 2254, 2430, 2409, 2636, 2509, 5771, 5761, 1362, 1227, 585, 1395, 2309, 3215, 5437, 516, 3364, 5192, 5323, 3893, 5393, 5215, 2260, 5059, 3922, 4288, 4945, 2729, 5135, 1369, 5595, 2464, 1534, 1754, 5548, 2644, 3181, 2882, 2889, 5390, 3441, 3481, 3501, 1780, 1956, 2859, 4595, 3541, 940, 3483, 2378, 5272, 3165, 734, 5641, 2305, 3145, 1004, 3370, 2966, 4326, 801, 4195, 452, 1921, 3585, 2284, 3952, 3775, 5702, 4531, 3790, 3749, 4616, 58, 3367, 1922, 3316, 1340, 925, 3761, 2686, 5260, 2109, 2083, 2997, 5055, 4903, 5770, 2195, 5189, 4265, 4888, 56, 2443, 1787, 4801, 4623, 641, 2424, 5370, 539, 1000, 3380, 3793, 141, 2332, 4297, 3313, 4036, 4448, 2860, 5347, 4273, 2110, 3504, 2980, 2253, 3554, 4023, 5444, 481, 3999, 3353, 1375, 716, 3319, 1888, 686, 221, 2608, 609, 4970, 3219, 30, 4525, 1197, 162, 2855, 5204, 5170, 2614, 963, 4617, 698, 4004, 3738, 510, 1683, 4894, 1964, 3951, 5612, 3171, 2197, 1437, 4743, 3275, 113, 814, 244, 4916, 2984, 382, 960, 324, 4830, 20, 3126, 3723, 4574, 2863, 5484, 1934, 3004, 828, 4785, 1554, 2623, 4089, 883, 2606, 3423, 4146, 1170, 2564, 4120, 97, 250, 3647, 1590, 4958, 3159, 80, 4641, 4110, 1744, 5320, 2249, 5732, 826, 5376, 1653, 4109, 3274, 272, 1468, 2330, 851, 2098, 179, 838, 2447, 4166, 1903, 432, 2324, 1290, 1251, 3691, 1617, 2198, 5043, 4678, 1694, 3119, 5283, 3833, 4018, 4113, 4856, 147, 2375, 897, 825, 4553, 4188, 3268, 5500, 3413, 4951, 3033, 1803, 3995, 3167, 1515, 4276, 4201, 752, 807, 2877, 457, 3773, 4857, 1816, 5310, 5512, 5599, 4606, 5201, 2986, 2720, 482, 2703, 348, 5777, 5549, 2862, 4454, 1372, 3190, 2634, 3063, 2483, 841, 2499, 2459, 4588, 1266, 3670, 4599, 1334, 4024, 5328, 1308, 5459, 5105, 64, 4848, 4041, 721, 4181, 1509, 3782, 593, 3352, 880, 1112, 2126, 2039, 3425, 4967, 3822, 4665, 1728, 5643, 3142, 5006, 4799, 5091, 385, 2236, 4327, 2189, 4745, 446, 3277, 1033, 5530, 748, 571, 1957, 5391, 2231, 4643, 2867, 2043, 1365, 5293, 3166, 5117, 5261, 4809, 484, 1444, 1819, 1806, 5677, 131, 703, 5661, 4095, 5209, 3865, 5288, 1320, 2660, 2678, 5157, 1050, 199, 3373, 4, 3205, 4015, 2805, 3681, 869, 4370, 3206, 5724, 5022, 924, 4368, 5609, 5532, 796, 2878, 2905, 3851, 3601, 1940, 4157, 1526, 4252, 4686, 1859, 5127, 1517, 3376, 3873, 5590, 5759, 3516, 4651, 699, 3226, 5278, 1681, 4815, 4051, 3582, 3058, 5433, 38, 5441, 1569, 5481, 1875, 3155, 1355, 49, 647, 1538, 5482, 5210, 2779, 4399, 1765, 1722, 1821, 3727, 4429, 3523, 5585, 2957, 570, 4875, 3535, 831, 5342, 4298, 2996, 984, 3947, 1076, 166, 1337, 5740, 4999, 5501, 1690, 4596, 424, 2969, 3772, 5616, 2876, 4038, 4974, 4343, 3891, 4366, 476, 534, 5171, 5673, 4410, 5245, 1476, 1336, 3813, 3068, 4934, 5164, 1730, 2917, 4156, 430, 1784, 643, 2469, 5035, 4130, 4136, 5632, 4962, 283, 2887, 3976, 637, 2180, 2340, 3254, 5411, 4891, 4414, 3713, 2538, 1791, 366, 2761, 2919, 2651, 35, 170, 3522, 2466, 5344, 2751, 969, 5637, 2112, 2432, 3646, 2955, 3032, 4597, 3479, 3968, 2226, 2344, 5081, 4255, 4206, 235, 5174, 4702, 5037, 4167, 4798, 1654, 697, 868, 2503, 4378, 1915, 2145, 4975, 4165, 864, 3754, 3405, 4141, 4359, 1052, 3375, 74, 3082, 2366, 3895, 3230, 2593, 1231, 1581, 1029, 3900, 1118, 1699, 4684, 1969, 5681, 4316, 1314, 1633, 3545, 1458, 3879, 4093, 279, 789, 5405, 5316, 1777, 3996, 4025, 3562, 1066, 276, 5780, 5438, 1782, 267, 5557, 3930, 3787, 1487, 332, 4886, 1318, 3264, 2771, 4473, 5294, 1184, 5358, 2881, 90, 57, 4222, 3130, 5665, 305, 2156, 3984, 1208, 3870, 3363, 1263, 5185, 2394, 5472, 572, 4829, 2064, 3468, 5535, 437, 94, 2797, 2163, 3575, 4490, 2277, 2183, 2683, 5600, 2902, 3804, 4444, 1012, 2323, 3527, 549, 2294, 978, 4481, 985, 3097, 142, 3095, 4172, 4173, 5181, 5335, 3596, 3770, 4302, 579, 2928, 649, 2244, 4325, 1090, 2063, 2833, 395, 4266, 4472, 2190, 3478, 5307, 2645, 1583, 5092, 5453, 929, 2707, 1400, 2785, 3073, 799, 3056, 2613, 1631, 4434, 4328, 3310, 1479, 4453, 347, 2041, 3890, 1692, 4714, 4841, 2108, 73, 2054, 2569, 4558, 2528, 4939, 3654, 1709, 3903, 736, 5583, 5630, 5100, 1894, 4867, 3768, 4584, 124, 1609, 4421, 518, 4275, 2857, 2926, 176, 112, 712, 1630, 353, 4170, 1627, 1481, 926, 1057, 5056, 1233, 5178, 2959, 2450, 2473, 415, 4615, 4121, 5722, 4646, 2391, 1932, 2101, 2825, 3170, 375, 2684, 544, 5594, 4372, 3932, 1454, 3385, 5254, 765, 2885, 5113, 4912, 873, 4837, 1096, 4260, 2517, 2065, 4654, 4283, 4031, 998, 3978, 291, 3503, 2022, 2505, 766, 602, 2960, 1624, 1298, 4846, 5670, 1130, 3643, 3134, 2399, 4859, 3634, 4039, 782, 2129, 294, 4701, 5570, 5582, 3287, 3619, 1205, 972, 4400, 3808, 4688, 5410, 4834, 3789, 1245, 2745, 5522, 2909, 2496, 3038, 3105, 992, 5635, 448, 3664, 2635, 1048, 78, 5432, 2246, 886, 304, 25, 3322, 4486, 2788, 2674, 3151, 2027, 1106, 1644, 1552, 3662, 2513, 2037, 5521, 4512, 4192, 5024, 2755, 3801, 3926, 220, 2809, 5144, 1898, 921, 3197, 709, 931, 370, 3998, 5479, 1912, 4871, 3826, 1938, 5493, 333, 832, 3909, 5242, 24, 2515, 1270, 2137, 5607, 5434, 3362, 1287, 1685, 2282, 4980, 5287, 1724, 3418, 3154, 4614, 2449, 160, 5225, 5258, 4164, 4102, 563, 3539, 612, 616, 403, 2150, 2904, 4771, 3147, 2343, 4353, 4152, 4733, 756, 4839, 3931, 2177, 2989, 4906, 4543, 3372, 2131, 3848, 3526, 2080, 2017, 761, 1943, 4529, 380, 4724, 1550, 2187, 4319, 21, 2511, 2179, 5231, 2265, 2642, 4377, 4231, 5057, 2373, 1604, 1098, 4921, 3894, 3519, 1080, 3102, 1069, 2191, 3811, 4707, 2479, 1700, 5766, 3030, 3622, 2379, 3852, 1450, 4578, 5772, 1929, 3106, 52, 3771, 4997, 4985, 2793, 3974, 3751, 1880, 1330, 1432, 1949, 4364, 1300, 3573, 226, 1706, 5758, 4825, 2689, 2168, 1740, 2910, 1763, 1682, 5387, 1524, 3356, 1470, 4228, 4463, 4995, 3985, 3135, 497, 5622, 4475, 3075, 4534, 218, 3443, 1183, 4318, 2938, 3339, 3628, 3378, 1693, 1017, 2875, 3747, 3282, 4474, 1619, 2942, 2420, 3117, 2995, 2531, 660, 3636, 2416, 1863, 5505, 3255, 1480, 3398, 5452, 3064, 1181, 1674, 5119, 3907, 941, 2733, 4626, 4573, 1026, 2247, 2916, 3557, 3649, 5428, 2239, 1871, 1844, 1616, 9, 2890, 737, 4993, 1591, 5606, 1799, 4207, 5298, 3412, 2461, 125, 2943, 5193, 1917, 603, 1139, 595, 665, 1092, 3224, 2646, 5567, 3812, 1498, 2983, 631, 692, 3286, 5401, 4994, 3697, 3213, 805, 5699, 3704, 3781, 834, 4722, 3763, 3600, 1768, 5063, 2828, 4699, 422, 2338, 733, 4899, 2030, 1794, 2945, 3797, 575, 3292, 4068, 5435, 5545, 1084, 648, 2588, 1168, 2717, 461, 2385, 2577, 2848, 2936, 398, 5475, 3406, 3284, 3711, 3482, 5207, 3017, 5542, 3916, 5547, 1036, 3138, 3680, 5134, 552, 2690, 3992, 2290, 4002, 1383, 1213, 5419, 2570, 393, 2481, 2016, 917, 1058, 4736, 4816, 4498, 5138, 1392, 495, 1701, 478, 2758, 1262, 5373, 2417, 3148, 5697, 1345, 4134, 3157, 912, 1399, 934, 1278, 2691, 5141, 696, 3698, 4561, 397, 3049, 5415, 454, 1099, 1065, 3112, 2157, 2778, 577, 5078, 3875, 2199, 217, 1879, 515, 1250, 843, 3878, 1147, 527, 3420, 3946, 5239, 5089, 5592, 3007, 2719, 77, 384, 954, 1835, 5602, 2655, 1729, 18, 4380, 5730, 2728, 306, 2962, 2772, 3131, 2252, 2398, 5523, 5176, 3196, 2947, 1713, 5462, 4484, 3631, 1632, 3042, 538, 1459, 1501, 3827, 4530, 168, 297, 2666, 2568, 2566, 5129, 1364, 3843, 3381, 5109, 5499, 3110, 997, 4845, 5741, 5162, 1477, 193, 2149, 412, 4341, 2553, 3623, 5068, 2968, 2045, 1920, 2227, 2152, 5229, 3876, 5110, 5384, 1411, 5155, 950, 4161, 1512, 994, 3524, 859, 1770, 3333, 3726, 2933, 2818, 1937, 5364, 2854, 203, 4496, 3100, 4689, 866, 3732, 4229, 5682, 5534, 1159, 4863, 4119, 4096, 4693, 4365, 5296, 1707, 1721, 1862, 1866, 3430, 4706, 5036, 2020, 1762, 1848, 726, 1584, 2523, 453, 2596, 642, 1288, 2319, 3954, 2436, 5768, 2741, 3293, 2664, 1991, 494, 1310, 4915, 381, 1054, 4953, 1185, 3127, 2285, 3703, 2901, 4581, 5371, 2549, 4563, 2315, 4780, 2307, 845, 4562, 5235, 1123, 5088, 1019, 3120, 701, 2328, 91, 944, 2458, 2546, 3708, 3399, 1602, 3480, 1737, 4138, 390, 4299, 5375, 772, 920, 4732, 4667, 1384, 4107, 2992, 1148, 2350, 1927, 5779, 4412, 1121, 5251, 5289, 4990, 2575, 4764, 3161, 4878, 5096, 357, 1939, 776, 4552, 2907, 2185, 372, 1620, 5348, 3889, 4021, 4730, 3831, 2649, 2844, 4988, 4040, 1828, 2590, 927, 5474, 1292, 4528, 4013, 4831, 798, 1647, 918, 3, 2542, 5586, 4602, 2815, 4671, 3589, 5205, 1788, 4727, 4713, 1276, 5010, 522, 3612, 5160, 3507, 5489, 5399, 928, 4511, 2205, 1483, 5764, 1638, 1373, 3103, 1414, 5085, 1577, 159, 1887, 970, 4784, 5001, 5716, 3136, 4919, 4944, 404, 402, 2605, 3693, 2302, 5133, 3401, 3185, 4613, 1081, 887, 1401, 4762, 2317, 1085, 5161, 2697, 2454, 2821, 1961, 2952, 815, 4020, 4832, 5680, 2044, 5539, 5638, 2455, 1593, 4012, 2357, 5454, 5199, 1167, 4542, 821, 2211, 5097, 974, 785, 4086, 2255, 1253, 1249, 3834, 1785, 713, 4057, 4536, 1268, 2718, 5366, 68, 5684, 1062, 489, 5026, 655, 1671, 2680, 3650, 3553, 5041, 661, 604, 5396, 79, 5394, 2326, 4862, 1726, 1381, 1124, 5579, 3544, 2628, 3204, 2571, 5064, 3066, 3118, 4924, 2804, 5072, 1760, 2520, 119, 201, 2903, 5032, 1174, 4309, 4355, 2979, 3424, 810, 5101, 445, 5753, 5017, 556, 183, 329, 1864, 361, 469, 4209, 3733, 243, 3010, 3432, 4289, 53, 2561, 4350, 2408, 3144, 957, 3606, 3943, 2489, 3411, 5354, 3450, 1360, 2471, 1045, 261, 5620, 3948, 1941, 4409, 2158, 5693, 5195, 4566, 5379, 1171, 670, 3767, 225, 4324, 4227, 3018, 69, 2032, 5069, 4991, 1568, 4357, 4650, 3671, 4540, 5672, 1811, 2321, 4242, 2347, 1914, 1746, 4382, 4340, 1645, 565, 280, 2269, 5302, 2625, 2419, 862, 4750, 1582, 3006, 3231, 4768, 3490, 3610, 4000, 3348, 1852, 4184, 3146, 4361, 4630, 4124, 5611, 354, 1346, 558, 300, 3359, 4489, 2346, 3584, 3229, 75, 5634, 4708, 3053, 4320, 3568, 3923, 4362, 2763, 519, 3238, 890, 5276, 4911, 4442, 4253, 3898, 1380, 3615, 1225, 2894, 2204, 4824, 946, 4313, 4337, 4408, 2812, 5604, 615, 335, 2007, 3823, 310, 33, 2627, 3941, 5737, 138, 989, 5015, 858, 3087, 72, 5137, 591, 1936, 411, 1241, 5284, 2397, 5531, 134, 5469, 1885, 4941, 632, 668, 1460, 500, 3437, 1640, 2787, 136, 3225, 2335, 4777, 2824, 3187, 895, 1561, 3334, 1389, 3387, 2274, 2897, 635, 451, 164, 4715, 5012, 4074, 5562, 1105, 3532, 5580, 3569, 1572, 1686, 5315, 4554, 5674, 5581, 2735, 2362, 5007, 3637, 2906, 2405, 526, 4591, 3455, 2941, 910, 5439, 2820, 1311, 4861, 2493, 1634, 5060, 513, 964, 4236, 5108, 5337, 1992, 618, 2270, 907, 3915, 440, 4314, 270, 106, 2172, 4332, 3415, 3736, 5386, 4579, 2318, 2563, 1070, 4852, 535, 1528, 1776, 172, 723, 2220, 1016, 1790, 2773, 2801, 2214, 1623, 196, 4765, 5267, 4290, 1210, 5080, 3078, 3463, 259, 911, 1442, 4416, 190, 5491, 3508, 4658, 1594, 5325, 3019, 3581, 5270, 2537, 3234, 2169, 919, 5226, 5593, 2921, 195, 1071, 2009, 5291, 3774, 3877, 3641, 3577, 3495, 5416, 1603, 317, 2081, 3720, 659, 1772, 1110, 822, 4695, 3345, 3629, 672, 4162, 2313, 99, 3920, 1035, 3280, 2228, 724, 3702, 3250, 2510, 5013, 2390, 5058, 2886, 4564, 5738, 4155, 3917, 184, 2970, 3074, 198, 1858, 923, 2465, 3513, 3034, 4869, 5573, 4826, 4926, 3841, 1527, 5198, 3674, 2592, 3116, 2349, 2830, 2066, 5122, 3627, 1546, 101, 1913, 2056, 3003, 1717, 4507, 5280, 334, 5140, 3000, 3737, 5203, 4381, 1951, 1014, 48, 5640, 1545, 2048, 2264, 4032, 613, 4940, 4210, 627, 829, 2146, 4116, 3474, 5577, 3281, 1293, 2026, 3427, 3085, 2088, 1820, 2622, 1018, 720, 3469, 5529, 1032, 1103, 1541, 791, 1606, 1422, 311, 2440, 3640, 1712, 906, 4425, 4955, 4909, 3447, 3123, 3098, 5649, 3320, 3295, 2710, 2594, 441, 214, 5471, 5707, 4805, 4521, 1446, 5274, 1100, 1186, 2154, 4547, 3785, 5623, 4674, 955, 60, 5627, 5658, 2096, 1895, 3994, 2667, 1261, 5079, 234, 1500, 2217, 5039, 5317, 88, 3016, 2918, 1549, 5666, 1255, 4111, 2135, 4028, 146, 2341, 1240, 597, 689, 4608, 947, 5244, 4748, 1996, 3668, 1598, 4030, 2353, 1532, 4670, 5368, 4345, 465, 1089, 3104, 3302, 5200, 3174, 2676, 4843, 3794, 2534, 4334, 2327, 4008, 3656, 2507, 3599, 2023, 5360, 3857, 2401, 809, 1976, 438, 1530, 1510, 2033, 2068, 3407, 2617, 3552, 1011, 3311, 4948, 2794, 2258, 3122, 5647, 3721, 4285, 1853, 3593, 3859, 914, 691, 4010, 2873, 3408, 3026, 2657, 5165, 3980, 1010, 824, 4663, 4471, 4492, 3341, 4813, 2927, 827, 4180, 1406, 559, 1719, 1758, 2125, 2548, 2601, 758, 1425, 2062, 3644, 4976, 2739, 2029, 1965, 5667, 2819, 2633, 2946, 1350, 3201, 4927, 4810, 115, 1193, 2134, 149, 5426, 4233, 1695, 3297, 5756, 5760, 5002, 1952, 5546, 1319, 2383, 854, 2173, 1443, 4097, 5644, 2381, 3525, 2203, 904, 2565, 1897, 2215, 4072, 540, 2071, 4660, 5527, 377, 3466, 1216, 3666, 2018, 180, 2462, 2480, 161, 3660, 173, 5264, 3445, 5619, 4460, 363, 3221, 1478, 1020, 3842, 2435, 3502, 237, 3881, 3648, 3022, 2993, 1757, 3039, 669, 4139, 1109, 1286, 2583, 1044, 3728, 2374, 2961, 4495, 4226, 3300, 1673, 2987, 601, 5382, 4928, 1523, 4739, 2463, 4145, 2872, 4703, 1845, 1715, 3755, 2308, 4726, 875, 3626, 573, 3029, 4339, 320, 3442, 50, 5566, 836, 4935, 239, 3108, 1994, 275, 4897, 1558, 614, 3586, 3938, 1786, 4480, 4033, 4795, 3434, 3769, 4598, 5217, 356, 4986, 4106, 3971, 1851, 3839, 857, 4262, 4779, 4964, 3023, 1769, 5319, 1188, 4868, 2295, 1254, 953, 1751, 4174, 62, 3798, 5206, 4624, 4238, 5749, 2092, 1665, 2883, 3141, 318, 1307, 1548, 3958, 419, 413, 5657, 3296, 5626, 1974, 4860, 3565, 2000, 2021, 2262, 3175, 3571, 85, 2234, 1236, 4515, 3709, 4046, 4933, 1970, 1244, 4426, 4783, 1260, 5027, 5333, 523, 1077, 652, 1127, 5720, 5608, 2711, 1789, 4719, 5652, 2567, 1767, 5050, 4014, 3757, 5343, 4367, 427, 2502, 4822, 2775, 2372, 5449, 4704, 2504, 1407, 5610, 4065, 2737, 576, 1447, 3880, 3475, 1462, 3692, 3163, 611, 4882, 4196, 1209, 3497, 4510, 5253, 3054, 2740, 5603, 3040, 1978, 1993, 1279, 4757, 4243, 5538, 1747, 2893, 1333, 2971, 2977, 189, 485, 5569, 3661, 1779, 3431, 255, 2386, 3687, 1573, 848, 4168, 1838, 717, 4118, 870, 4269, 3101, 4505, 3988, 2019, 3487, 3884, 4402, 863, 1963, 2555, 687, 2261, 1239, 1843, 3892, 5033, 995, 835, 369, 666, 4501, 4356, 794, 2610, 3914, 4502, 4144, 2834, 5149, 477, 5669, 4820, 2413, 5731, 15, 3655, 1586, 1537, 3979, 1495, 2823, 2695, 2895, 2944, 993, 4594, 5266, 543, 4696, 2446, 3340, 2838, 3059, 367, 5190, 4084, 2412, 3776, 654, 3089, 1916, 2638, 4081, 5228, 4386, 4395, 5322, 4576, 2915, 431, 2784, 2620, 4267, 4413, 5515, 5717, 5186, 2780, 778, 312, 600, 3168, 5713, 1743, 5142, 1738, 3665, 1566, 2008, 5467, 1702, 4248, 530, 1047, 633, 396, 4190, 4058, 3969, 5395, 2076, 4270, 4257, 5763, 1971, 2237, 2998, 505, 1056, 2641, 2699, 5558, 5065, 4445, 4220, 4007, 606, 5598, 1248, 3566, 2536, 991, 2376, 2767, 4469, 2279, 3613, 1410, 1521, 4518, 3465, 405, 4069, 747, 3014, 2880, 1881, 1781, 5378, 5711, 1945, 5145, 3124, 2524, 3810, 882, 1555, 223, 3910, 3609, 561, 4440, 3993, 4186, 2677, 5664, 1259, 4681, 3379, 4638, 3549, 194, 5466, 1347, 1203, 781, 3454, 5303, 837, 1764, 1068, 7, 204, 1718, 3633, 3515, 3220, 34, 277, 4491, 102, 3326, 738, 3172, 4694, 3580, 2006, 5552, 1328, 2967, 4590, 1918, 3594, 2120, 2925, 589, 231, 5318, 1234, 4858, 202, 123, 5502, 2267, 3491, 525, 1285, 5497, 122, 3351, 339, 1643, 5533, 4587, 4835, 4027, 3494, 2764, 3270, 937, 1710, 1874, 3591, 739, 4842, 1235, 3383, 2580, 4996, 598, 5367, 71, 2453, 581, 5061, 3625, 5083, 2011, 2597, 4151, 4133, 4060, 5494, 5781, 5550, 5516, 2914, 3048, 3256, 4932, 1980, 2140, 3816, 2059, 1196, 2363, 5486, 4752, 1837, 3336, 5218, 4384, 1277, 2958, 2418, 1691, 351, 3266, 1023, 2165, 5543, 5153, 2746, 486, 1421, 3208, 1575, 89, 4758, 3186, 4827, 4946, 2937, 777, 40, 2659, 2388, 5107, 5219, 2431, 207, 4666, 4808, 5578, 488, 3133, 813, 4957, 2692, 4079, 4467, 4126, 4885, 4271, 1892, 4527, 5038, 5023, 1416, 400, 1496, 2497, 2852, 537, 1153, 3997, 5656, 4751, 550, 2540, 3688, 4917, 4585, 719, 4920, 156, 1073, 1091, 1649, 1198, 5742, 1061, 5464, 4735, 2474, 2482, 4306, 232, 3079, 1741, 3294, 4303, 1663, 1064, 1247, 1075, 251, 1354, 2949, 3249, 3605, 1535, 2070, 4247, 4268, 1452, 3911, 2908, 4193, 5689, 5148, 1589, 1022, 4544, 3598, 2587, 5733, 634, 126, 266, 3624, 5574, 343, 2670, 1985, 830, 3977, 1067, 2495, 4537, 3651, 901, 2619, 4849, 1222, 1034, 3467, 1082, 3743, 5712, 3567, 2631, 118, 1471, 744, 768, 2047, 4418, 2723, 1899, 4394, 2701, 3858, 349, 1736, 4446, 1143, 5496, 2414, 3077, 5216, 1377, 2871, 4545, 2060, 3137, 959, 4669, 1256, 4342, 2525, 2640, 3799, 3840, 2380, 1733, 3597, 679, 5021, 5372, 3869, 2498, 5440, 2637, 1511, 3837, 1423, 3913, 1924, 1928, 3500, 5040, 4427, 2835, 3081, 2687, 4740, 5675, 1796, 39, 4351, 3459, 5297, 4063, 4711, 2490, 278, 1150, 3164, 5034, 2681, 2342, 154, 3177, 2765, 3780, 3942, 5639, 1321, 3868, 5510, 728, 371, 2850, 5237, 2188, 2278, 5196, 3815, 746, 3746, 2600, 4794, 2522, 4183, 1353, 2554, 5776, 3928, 4482, 408, 5241, 2675, 3440, 1752, 1833, 4806, 4055, 1502, 4140, 4879, 3807, 5400, 5725, 5159, 4406, 1547, 2586, 2972, 1967, 5769, 3111, 4401, 3861, 2114, 4819, 4251, 2832, 4213, 4499, 1482, 1357, 3863, 406, 4618, 4755, 3734, 512, 1030, 336, 5715, 4532, 4776, 5487, 4419, 5406, 3611, 3337, 982, 4541, 2526, 116, 2089, 462, 3262, 750, 1797, 5408, 1734, 321, 532, 1771, 1998, 5678, 5000, 3559, 110, 4487, 678, 1403, 2207, 1867, 2361, 3973, 4122, 3547, 4433, 4893, 2604, 3982, 4131, 1059, 3929, 1508, 3349, 3604, 2985, 3374, 787, 4870, 5418, 2708, 727, 2100, 2184, 753, 5125, 3814, 4205, 3602, 2015, 1402, 4333, 5112, 3416, 4241, 4503, 3283, 5030, 501, 5560, 5551, 988, 5071, 4818, 1592, 3488, 3829, 1870, 378, 2734, 4143, 5383, 4549, 120, 871, 2647, 5365, 3927, 2467, 708, 5718, 1388, 319, 3652, 2085, 286, 5306, 4635, 1359, 2954, 3244, 5019, 59, 208, 1433, 3695, 4570, 2425, 1275, 1597, 5066, 5313, 10, 2796, 5052, 5380, 1580, 4083, 2826, 4114, 2856, 436, 646, 3546, 624, 3972, 487, 5179, 760, 877, 2956, 546, 187, 817, 1587, 2802, 1636, 1990, 4677, 2433, 4632, 45, 2557, 2072, 4572, 3236, 5584, 861, 1688, 257, 4914, 3705, 4468, 2052, 2113, 2460, 4315, 1960, 401, 4840, 2808, 1942, 4760, 42, 144, 2130, 1658, 1506, 4778, 5528, 4092, 2002, 5520, 3062, 3346, 1398, 4633, 4422, 1574, 5591, 2370, 658, 651, 996, 1037, 3090, 4245, 37, 3520, 5613, 5442, 626, 1152, 1252, 421, 63, 5736, 1144, 4230, 5076, 4983, 1910, 2257, 4717, 3384, 1385, 2912, 4639, 5005, 3517, 2851, 3198, 4250, 362, 3753, 958, 281, 4349, 3028, 3686, 5166, 3845, 492, 3257, 531, 3448, 1832, 2451, 17, 2602, 1727, 3888, 3271, 569, 2434, 5086, 3792, 3390, 676, 108, 503, 4710, 358, 1379, 4077, 5503, 2730, 1046, 1618, 599, 2807, 16, 636, 1436, 5708, 707, 1352, 3227, 542, 4901, 265, 5389, 13, 2643, 4828, 342, 1850, 2551, 3114, 95, 5047, 644, 4803, 4439, 3178, 2240, 5049, 2176, 3121, 316, 4600, 4049, 1083, 3956, 260, 87, 379, 564, 1761, 908, 1544, 3279, 5263, 673, 1361, 932, 889, 4404, 4217, 2457, 3308, 705, 5540, 4804, 1257, 1027, 1146, 2476, 4959, 3305, 4415, 582, 1655, 4169, 567, 2091, 1982, 5281, 1218, 2939, 1716, 5734, 4774, 1451, 892, 2654, 878, 1164, 350, 5537, 574, 3896, 1438, 271, 433, 4148, 5407, 4884, 1749, 3360, 4001, 4175, 4101, 1093, 1176, 1472, 4088, 560, 1571, 5413, 2626, 1177, 206, 2822, 3200, 945, 2727, 650, 5082, 4604, 2297, 842, 3217, 3330, 2532, 681, 264, 3872, 5456, 2582, 442, 4447, 2367, 3579, 4627, 4742, 5597, 5338, 1323, 107, 4892, 2896, 5524, 1834, 414, 2028, 909, 816, 5696, 5090, 4523, 5255, 5020, 27, 4636, 1116, 1981, 1703, 5073, 3712, 2097, 4137, 1101, 3377, 2584, 4115, 360, 1611, 1839, 620, 780, 365, 1997, 2371, 3222, 3682, 2892, 4556, 5721, 3096, 5553, 1228, 1435, 163, 5331, 1588, 3710, 4890, 2209, 1316, 4047, 5754, 4441, 1836, 2491, 966, 4865, 2679, 233, 1404, 1301, 2700, 4112, 4423, 3669, 4417, 2232, 4393, 3630, 3076, 5377, 2142, 930, 3838, 5747, 3245, 1461, 935, 2259, 3830, 148, 1180, 3498, 715, 690, 5046, 1827, 3129, 528, 5256, 3730, 132, 3885, 5388, 5451, 464, 4026, 740, 289, 2410, 4259, 2759, 1053, 1908, 5252, 2884, 5650, 4895, 2141, 850, 4546, 1246, 1449, 2242, 623, 5404, 5004, 2963, 1358, 2166, 3191, 802, 3176, 2845, 3184, 2831, 1666, 1042, 508, 2031, 268, 2428, 557, 5421, 171, 590, 5445, 3860, 5238, 1595, 811, 4080, 4685, 653, 2107, 4855, 1156, 4797, 4374, 2094, 2298, 1491, 2827, 5509, 3786, 1305, 586, 2810, 4071, 1122, 4483, 3369, 2913, 3820, 5163, 5465, 2782, 4516, 4812, 2001, 1243, 3788, 302, 67, 3672, 5183, 219, 4910, 1909, 645, 3457, 245, 3248, 1684, 4016, 2508, 5150, 1072, 5009, 4457, 1579, 3578, 5544, 3419, 5212, 3904, 2581, 1003, 793, 1723, 4611, 2973, 143, 551, 3883, 4003, 3871, 5431, 3312, 1670, 2535, 1129, 596, 3970, 4712, 5233, 2077, 1060, 459, 151, 455, 2837, 5526, 5340, 4295, 3919, 764, 2521, 2754, 4952, 952, 865, 1807, 2738, 2281, 4731, 386, 2547, 4274, 3663, 5182, 1108, 301, 2770, 5172, 3489, 729, 2842, 3301, 4877, 1313, 3128, 3099, 1818, 622, 2792, 2817, 5698, 5783, 5227, 1660, 5194, 2218, 4464, 5309, 298, 3509, 1063, 1178, 5250, 3521, 4296, 1748, 4054, 282, 3259, 5422, 19, 3182, 423, 1783, 5443, 1668, 2694, 2132, 3477, 28, 683, 4559, 3047, 605, 3735, 3856, 5381, 1878, 4514, 376, 5223, 3020, 2271, 1801, 4729, 341, 2387, 1269, 1051, 1024, 1656, 5350, 3560, 2224, 1753, 2630, 5752, 2924, 2365, 3093, 1297, 5687, 1973, 1111, 625, 2930, 2122, 3725, 1273, 3963, 3803, 2422, 855, 3143, 3484, 1140, 1466, 2316, 4142, 2742, 4391, 29, 4300, 587, 3556, 4397, 3276, 1642, 1999, 4329, 209, 2769, 5175, 3944, 2128, 3193, 1304, 2178, 3240, 4557, 3361, 1232, 1675, 4675, 5662, 152, 5409, 2760, 3041, 4538, 5044, 4979, 1324, 491, 4125, 1900, 4586, 4513, 4744, 2953, 881, 3357, 4050, 5561, 157, 4163, 4788, 4128, 1132, 4792, 5596, 2756, 1652, 5130, 444, 470, 2448, 4705, 4866, 249, 4690, 4902, 121, 5653, 1830, 4044, 1739, 1378, 1872, 3027, 496, 2665, 2219, 2744, 4500, 3905, 4676, 1315, 1664, 779, 2283, 2263, 1629, 5511, 1299, 1876, 3676, 4388, 5116, 4127, 1040, 5615, 5417, 3699, 2501, 4567, 1679, 4662, 2303, 182, 820, 1869, 1877, 2400, 175, 1639, 4189, 1824, 3460, 1457, 3158, 3461, 4062, 2865, 2948, 682, 4129, 5018, 5618, 3744, 2186, 242, 2306, 4836, 714, 344, 4182, 2950, 5498, 4178, 3263, 4539, 1192, 2268, 554, 536, 186, 1808, 786, 4560, 368, 3570, 3470, 5425, 1613, 3675, 2541, 788, 475, 774, 4234, 2337, 3444, 2377, 153, 3433, 3394, 3402, 2572, 1086, 3389, 5719, 2025, 4746, 11, 1513, 4435, 1295, 2266, 2762, 1925, 4006, 4185, 749, 2151, 876, 374, 3762, 1986, 1626, 3107, 3327, 968, 5014, 128, 1516, 328, 4791, 2339, 3195, 4734, 1563, 812, 1317, 2121, 541, 663, 2598, 5601, 4913, 2816, 5257, 916, 1975, 2084, 4692, 4171, 3918, 1847, 967, 893, 5654, 2368, 4272, 2994, 1343, 5692, 4249, 340, 3115, 3325, 1097, 4176, 5363, 1805, 4992, 3355, 5167, 5714, 767, 2333, 3621, 3701, 1087, 4322, 3750, 3901, 3046, 4972, 3855, 426, 3550, 1689, 1612, 32, 3538, 4790, 4987, 3825, 3548, 1206, 735, 5576, 965, 5011, 3583, 5236, 2406, 3013, 5477, 449, 3543, 1426, 5361, 4064, 3303, 4907, 1931, 458, 3561, 2251, 1189, 840, 1134, 5572, 155, 2843, 4147, 1136, 5224, 4223, 4782, 5628, 704, 5513, 5565, 1680, 5132, 3700, 4659, 4601, 2, 4770, 3080, 1, 47, 66, 4224, 4458, 1348, 5430, 922, 169, 2082, 899, 4493, 1766, 230, 4293, 5295, 2715, 5341, 3031, 1490, 939, 3109, 2673, 4216, 619, 399, 1854, 2940, 3854, 1930, 1339, 4718, 5476, 4292, 1677, 2013, 4462, 3795, 4625, 4449, 2709, 1163, 285, 3518, 2155, 578, 1434, 4279, 4661, 1223, 2058, 1935, 4640, 4697, 331, 4823, 4853, 629, 4938, 4208, 5234, 3683, 3531, 3740, 1001, 4593, 1440, 849, 3429, 2106, 4880, 5765, 4949, 1165, 5184, 1669, 2776, 2118, 4179, 4459, 3953, 1849, 3639, 4904, 1306, 3722, 2911, 3246, 2978, 3417, 4099, 4793, 4838, 913, 4123, 1370, 885, 1504, 583, 4082, 428, 3764, 2975, 4363, 2798, 150, 5710, 5536, 387, 3493, 1886, 1366, 4673, 2364, 5757, 4929, 664, 1142, 1551, 1272, 1191, 5392, 5374, 5645, 3595, 1224, 2136, 417, 1607, 3209, 2991, 1659, 3149, 5329, 2682, 185, 224, 4244, 1810, 2550, 2359, 5147, 483, 4851, 4883, 3438, 3036, 943, 5154, 1149, 3533, 819, 757, 5525, 979, 1025, 5461, 2427, 1855, 5146, 3344, 2042, 3458, 2492, 1525, 1600, 1696, 3959, 915, 416, 3555, 5695, 3510, 2705, 3603, 1610, 3849, 3069, 5246, 5177, 2280, 2104, 1522, 3717, 4075, 1445, 1212, 617, 5282, 990, 4336, 743, 1117, 3476, 3156, 4773, 860, 3492, 1505, 5102, 2990, 425, 3934, 2721, 5051, 5152, 5279, 467, 2648, 808, 3618, 1393, 2040, 1599, 4450, 2781, 2148, 1007, 2334, 303, 5304, 1294, 315, 1154, 4555, 5327, 3314, 3874, 671, 3832, 5663, 5701, 5265, 853, 1705, 2115, 2485, 2241, 4687, 158, 3955, 1211, 2437, 4698, 1331, 710, 2923, 4256, 5099, 1137, 3332, 1456, 5463, 2964, 3912, 3382, 1267, 327, 1284, 238, 4237, 2616, 5679, 6, 5345, 2384, 5191, 4642, 4191, 2621, 3819, 2652, 2355, 5016, 1503, 388, 3358, 4187, 2506, 3667, 1405, 2731, 5470, 5098, 4583, 3321, 1429, 4387, 1428, 4022, 3472, 4330, 4053, 4648, 3403, 2225, 4978, 2858, 1815, 130, 771, 4438, 4310, 4305, 3400, 3065, 44, 3846, 293, 2624, 3315, 330, 1890, 2329, 200, 1349, 373, 5275, 3908, 4281, 4158, 562, 1621, 5625, 2811, 4517, 2444, 4066, 1497, 2441, 2726, 4052, 4485, 322, 2442, 4103, 1356, 2869, 730, 4420, 1499, 3802, 5136, 607, 2322, 3005, 4011, 181, 1338, 1329, 2201, 1078, 4725, 2475, 31, 1325, 1979, 3745, 1989, 3818, 3261, 12, 903, 65, 3796, 5103, 1485, 4405, 4749, 5589, 1676, 2976, 3936, 1463, 2653, 1049, 1958, 3760, 3092, 5268, 3499, 4461, 3414, 5490, 1382, 1242, 338, 1966, 1374, 5385, 4918, 638, 1237, 1601, 1419, 1732, 630, 3052, 1792, 844, 4896, 1175, 2650, 2105, 4519, 4876, 4159, 262, 900, 2202, 3778, 4963, 3678, 711, 5, 2010, 2552, 1955, 5723, 4965, 3169, 248, 4098, 5054, 2395, 210, 4551, 4037, 2753, 5048, 3528, 84, 1363, 5782, 3207, 517, 1553, 4087, 1408, 3897, 466, 4218, 769, 1271, 4436, 3957, 3967, 4656, 4029, 2658, 1585, 54, 662, 731, 3298, 5075, 3945, 5648, 2348, 5703, 2230, 3037, 2276, 392, 4149, 3139, 818, 5156, 299, 5332, 879, 2988, 2117, 2899, 3192, 3391, 5269, 5478, 3011, 3422, 2853, 2484, 4925, 4864, 3933, 568, 2069, 4644, 4747, 4900, 1711, 804, 1720, 2749, 241, 394, 4723, 2618, 4682, 2193, 1079, 2161, 775, 5690, 3617, 3343, 1774, 288, 3035, 3267, 1494, 4847, 1672, 1214, 4789, 1987, 3211, 1667, 2981, 1708, 1009, 4160, 2836, 3742, 2411, 2310, 4577, 3015, 1518, 3199, 490, 1312, 1342, 3572, 5143, 3247, 2256, 3239, 4509, 2470, 5485, 2488, 1635, 2866, 1902, 4282, 3847, 26, 3044, 4477, 5197, 5131, 3025, 2354, 2578, 2288, 2736, 1038, 137, 4211, 1800, 4497, 5188, 795, 999, 3689, 104, 3558, 2389, 4379, 5735, 4787, 3614, 4854, 1190, 4428, 4073, 2530, 1160, 4950, 2429, 1637, 3323, 3981, 4215, 471, 3426, 5248, 5455, 4235, 5211, 4942, 742, 3542, 1376, 3386, 252, 307, 86, 3966, 4451, 4240, 1622, 3658, 3309, 3783, 1893, 4494, 2840, 2965, 2696, 4873, 5750, 2206, 1605, 337, 1813, 2086, 3084, 1396, 1948, 1430, 460, 1469, 1141, 1615, 640, 4984, 3574, 5128, 3587, 1567, 1094, 3024, 2046, 933, 4649, 2999, 1519, 4059, 4620, 1424, 4605, 5488, 1529, 4476, 656, 499, 1883, 1113, 3242, 2932, 3485, 3563, 2404, 3421, 2275, 4931, 5655, 3404, 5700, 3486, 3009, 4817, 2527, 1397, 1344, 1687, 2935, 5483, 2423, 2795, 4700, 2014, 684, 2669, 5311, 3439, 4981, 4622, 4821, 3260, 2320, 896, 3347, 2074, 2800, 2743, 3950, 1823, 3766, 5518, 4716, 1114, 2243, 2075, 5624, 5555, 3451, 1750, 188, 5686, 197, 4317, 2806, 5273, 391, 4373, 3228, 1291, 3906, 3685, 2299, 2849, 3638, 4470, 256, 3241, 4311, 755, 3684, 4753, 1865, 1906, 839, 2196, 355, 2229, 3216, 1157, 5519, 545, 2291, 2116, 1455, 2585, 3290, 5554, 4887, 3756, 2057, 3368, 389, 1120, 3214, 1841, 1562, 3393, 3307, 4721, 3091, 547, 4466, 4286, 4396, 4520, 2099, 192, 2632, 5292, 4061, 1614, 1417, 4709, 3218, 3086, 14, 693, 2847, 1926, 4346, 981, 867, 1977, 3278, 3088, 695, 4565, 1187, 2024, 2562, 473, 5458, 1831, 675, 5414, 2392, 790, 4923, 1133, 4814, 4212, 2438, 3409, 1204, 1891, 3083, 1889, 5369, 5045, 770, 2951, 1596, 1896, 3428, 2661, 3008, 1557, 5305, 784, 4432, 1031, 2174, 83, 4225, 2194, 4219, 145, 2004, 4664, 4264, 1104, 409, 2067, 359, 135, 566, 273, 211, 2192, 3777, 667, 174, 5642, 1650, 1448, 3864, 5646, 1102, 82, 2611, 3949, 1742, 1095, 4203, 5336, 1608, 3152, 4321, 3632, 5507, 3592, 364, 2841, 1427, 3758, 2752, 3258, 3342, 3836, 3410, 4005, 1950, 51, 1386, 4019, 2868, 3965, 2402, 1221, 5450, 856, 521, 228, 3534, 610, 5617, 4376, 1735, 1467, 5614, 3882, 253, 2766, 5728, 2369, 4465, 4645, 1953, 1697, 1911, 191, 2301, 4767, 783, 3696, 1473, 1954, 4580, 847, 3125, 4443, 5460, 4239, 4526, 4922, 1351, 5232, 4775, 4198, 3867, 1162, 504, 5778, 763, 4261, 111, 1368, 797, 2124, 434, 4478, 1207, 3223, 1283, 4431, 2311, 5457, 5773, 4653, 2223, 2439, 2245, 1492, 5398, 1755, 3335, 1678, 4954, 2182, 529, 2167, 2061, 295, 3564, 3512, 2160, 1972, 3530, 4263, 2559, 2629, 745, 1041, 3162, 751, 1155, 5042, 3452, 5180, 5668, 5397, 1507, 5517, 5213, 4603, 2714, 5429, 4100, 852, 3366, 139, 3716, 5008, 105, 3975, 4989, 2920, 498, 1131, 2789, 2181, 520, 833, 2456, 4280, 4621, 346, 956, 5126, 4200, 5359, 2722, 2512, 2518, 3180, 5285, 592, 962, 2656, 1995, 2358, 3365, 2050, 5436, 5676, 216, 2663, 4612]\n",
            "Train Dataset: 2892\n",
            "Valid Dataset: 2892\n",
            "Test Dataset: 2893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVI4cOLMrte2"
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfUDMCe8r2J4"
      },
      "source": [
        "AlexNet_model.classifier[6] = nn.Linear(4096, NUM_CLASSES)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpTnQ0U1sGbF"
      },
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "parameters_to_optimize = AlexNet_model.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtJ8U_mPsTiF",
        "outputId": "fd10535a-72fe-4a84-b022-e0d00630d303"
      },
      "source": [
        "# By default, everything is loaded to cpu\n",
        "AlexNet_model = AlexNet_model.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    AlexNet_model.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = AlexNet_model(images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step() "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/40, LR = [0.01]\n",
            "Step 0, Loss 4.954519271850586\n",
            "Step 10, Loss 1.656557559967041\n",
            "Starting epoch 2/40, LR = [0.01]\n",
            "Step 20, Loss 0.4971584677696228\n",
            "Starting epoch 3/40, LR = [0.01]\n",
            "Step 30, Loss 0.2320249229669571\n",
            "Starting epoch 4/40, LR = [0.01]\n",
            "Step 40, Loss 0.13505080342292786\n",
            "Starting epoch 5/40, LR = [0.01]\n",
            "Step 50, Loss 0.06703073531389236\n",
            "Starting epoch 6/40, LR = [0.01]\n",
            "Step 60, Loss 0.048042476177215576\n",
            "Starting epoch 7/40, LR = [0.01]\n",
            "Step 70, Loss 0.05130879953503609\n",
            "Starting epoch 8/40, LR = [0.01]\n",
            "Step 80, Loss 0.04810072109103203\n",
            "Starting epoch 9/40, LR = [0.01]\n",
            "Step 90, Loss 0.02084573730826378\n",
            "Starting epoch 10/40, LR = [0.01]\n",
            "Step 100, Loss 0.019974827766418457\n",
            "Starting epoch 11/40, LR = [0.01]\n",
            "Step 110, Loss 0.019663618877530098\n",
            "Step 120, Loss 0.020432494580745697\n",
            "Starting epoch 12/40, LR = [0.01]\n",
            "Step 130, Loss 0.02202167734503746\n",
            "Starting epoch 13/40, LR = [0.01]\n",
            "Step 140, Loss 0.02116966061294079\n",
            "Starting epoch 14/40, LR = [0.01]\n",
            "Step 150, Loss 0.012413475662469864\n",
            "Starting epoch 15/40, LR = [0.01]\n",
            "Step 160, Loss 0.007644437253475189\n",
            "Starting epoch 16/40, LR = [0.01]\n",
            "Step 170, Loss 0.014376319944858551\n",
            "Starting epoch 17/40, LR = [0.01]\n",
            "Step 180, Loss 0.012472685426473618\n",
            "Starting epoch 18/40, LR = [0.01]\n",
            "Step 190, Loss 0.008104193955659866\n",
            "Starting epoch 19/40, LR = [0.01]\n",
            "Step 200, Loss 0.013578888028860092\n",
            "Starting epoch 20/40, LR = [0.01]\n",
            "Step 210, Loss 0.00929957628250122\n",
            "Starting epoch 21/40, LR = [0.0001]\n",
            "Step 220, Loss 0.008120313286781311\n",
            "Step 230, Loss 0.008332107216119766\n",
            "Starting epoch 22/40, LR = [0.001]\n",
            "Step 240, Loss 0.021448243409395218\n",
            "Starting epoch 23/40, LR = [0.001]\n",
            "Step 250, Loss 0.01992671936750412\n",
            "Starting epoch 24/40, LR = [0.001]\n",
            "Step 260, Loss 0.014876771718263626\n",
            "Starting epoch 25/40, LR = [0.001]\n",
            "Step 270, Loss 0.011753428727388382\n",
            "Starting epoch 26/40, LR = [0.001]\n",
            "Step 280, Loss 0.009747520089149475\n",
            "Starting epoch 27/40, LR = [0.001]\n",
            "Step 290, Loss 0.006265375763177872\n",
            "Starting epoch 28/40, LR = [0.001]\n",
            "Step 300, Loss 0.00775667279958725\n",
            "Starting epoch 29/40, LR = [0.001]\n",
            "Step 310, Loss 0.004713572561740875\n",
            "Starting epoch 30/40, LR = [0.001]\n",
            "Step 320, Loss 0.006731245666742325\n",
            "Starting epoch 31/40, LR = [0.001]\n",
            "Step 330, Loss 0.005514133721590042\n",
            "Step 340, Loss 0.004842184484004974\n",
            "Starting epoch 32/40, LR = [0.001]\n",
            "Step 350, Loss 0.011507153511047363\n",
            "Starting epoch 33/40, LR = [0.001]\n",
            "Step 360, Loss 0.007291752845048904\n",
            "Starting epoch 34/40, LR = [0.001]\n",
            "Step 370, Loss 0.005838021636009216\n",
            "Starting epoch 35/40, LR = [0.001]\n",
            "Step 380, Loss 0.010402992367744446\n",
            "Starting epoch 36/40, LR = [0.001]\n",
            "Step 390, Loss 0.005998995155096054\n",
            "Starting epoch 37/40, LR = [0.001]\n",
            "Step 400, Loss 0.008342642337083817\n",
            "Starting epoch 38/40, LR = [0.001]\n",
            "Step 410, Loss 0.009204808622598648\n",
            "Starting epoch 39/40, LR = [0.001]\n",
            "Step 420, Loss 0.011232350021600723\n",
            "Starting epoch 40/40, LR = [0.001]\n",
            "Step 430, Loss 0.004768773913383484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdZYZOd4s2PS",
        "outputId": "dc1bf426-952e-4b3d-c063-675bf097608c"
      },
      "source": [
        "AlexNet_model = AlexNet_model.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "AlexNet_model.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(val_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = AlexNet_model(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(val_dataset))\n",
        "\n",
        "print('Validation Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 12/12 [00:07<00:00,  1.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8385200553250346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cqgOOIZ3914"
      },
      "source": [
        "# For possible data augmentation we can use different transformation\n",
        "torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
        "torchvision.transforms.functional.adjust_gamma(img: torch.Tensor, gamma: float, gain: float = 1)\n",
        "torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}